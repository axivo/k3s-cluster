#!/usr/bin/env python3

import argparse
import datetime
import json
import logging
import os
import platform
import re
import subprocess
import sys
import tarfile
import time
import math
import csv
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

try:
    from kubernetes import client, config as k8s_config
except ImportError:
    print("Required dependency not found. Please install with:")
    print("apt install python3-kubernetes")
    sys.exit(1)


class K3sArchitectureDetector:
    """
    Detects K3s architecture type (single-server or High Availability) and provides configuration details.

    This class identifies whether a K3s cluster is running in single-server mode or
    high-availability mode, and provides details about the configuration to allow
    monitoring tools to adjust their diagnostics appropriately.
    """

    # K3s configuration and service file paths
    K3S_CONFIG_FILE = "/etc/rancher/k3s/config.yaml"
    K3S_SERVICE_FILE = "/etc/systemd/system/k3s.service"

    def __init__(self, **kwargs):
        """
        Initialize the K3s architecture detector.

        Args:
            **kwargs: Configuration options including:
                - k8s_api: Kubernetes API client (CoreV1Api)
                - k3s_labels: K3sLabels instance
                - logger: Logger instance (optional)
        """
        self.k8s_api = kwargs.get('k8s_api')
        self.labels = kwargs.get('k3s_labels')
        self.logger = kwargs.get('logger') or logging.getLogger(__name__)

        # Architecture details
        self.architecture_type = "unknown"  # 'single-server' or 'high-availability'
        self.control_plane_nodes = []
        self.worker_nodes = []
        self.datastore_type = "unknown"     # 'sqlite', 'etcd', 'external' (mysql/postgres)
        self.datastore_endpoint = None
        self.is_ha = False
        self.is_etcd_embedded = False
        self.is_server_node = False         # Whether the current node is a server/control-plane

        # Detected components
        self.has_embedded_etcd = False
        self.has_external_db = False
        self.external_db_type = None

        # Detection status
        self.detection_complete = False
        self.detection_methods_used = []
        self.detection_errors = []

    def _detect_datastore_type(self) -> bool:
        """
        Detect the datastore type being used by K3s.
        Uses multiple detection strategies in sequence.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        try:
            # If we already determined the datastore type from process arguments, we're done
            if self.datastore_type not in ["unknown"]:
                return True

            # Try different detection strategies in sequence
            detection_strategies = [
                self._detect_from_config_file,
                self._detect_from_service_file,
                self._detect_from_filesystem,
                self._infer_from_node_count
            ]

            for strategy in detection_strategies:
                if strategy():
                    return True

            return True

        except Exception as e:
            self.logger.error(f"Failed to detect datastore type: {e}")
            self.detection_errors.append(f"Datastore detection failed: {str(e)}")
            return False

    def _detect_etcd_presence(self) -> bool:
        """
        Detect presence of etcd in the cluster.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        try:
            # Check for etcd pods in kube-system namespace
            pods = self.k8s_api.list_namespaced_pod(namespace="kube-system")

            has_etcd_pods = False
            for pod in pods.items:
                if "etcd" in pod.metadata.name:
                    has_etcd_pods = True
                    self.logger.info(f"Found etcd pod: {pod.metadata.name}")

                    # If we see etcd pods, cluster is likely using embedded etcd
                    self.has_embedded_etcd = True
                    self.is_etcd_embedded = True
                    self.is_ha = True

                    if self.datastore_type == "unknown":
                        self.datastore_type = "etcd"

            # Try to get etcd endpoints from kubernetes API
            try:
                # Check if we can access etcd metrics via API
                success, output = self._run_kubectl_command([
                    "kubectl", "get", "--raw", "/metrics"
                ])

                if success and "etcd_" in output:
                    self.logger.info("Found etcd metrics in Kubernetes API")
                    self.has_embedded_etcd = True
                    self.is_etcd_embedded = True
                    self.is_ha = True

                    if self.datastore_type == "unknown":
                        self.datastore_type = "etcd"
            except Exception as e:
                self.logger.debug(f"Error checking etcd metrics: {e}")

            return True

        except Exception as e:
            self.logger.error(f"Failed to detect etcd presence: {e}")
            self.detection_errors.append(f"Etcd detection failed: {str(e)}")
            return False

    def _detect_from_config_file(self) -> bool:
        """
        Try to detect datastore type from K3s config file.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        if not os.path.exists(self.K3S_CONFIG_FILE):
            return False

        try:
            with open(self.K3S_CONFIG_FILE, 'r') as f:
                config_content = f.read()

            # Look for datastore-endpoint in config
            endpoint_match = re.search(r'datastore-endpoint:\s*(.+)', config_content)
            if not endpoint_match:
                return False

            endpoint = endpoint_match.group(1).strip()
            self.datastore_endpoint = endpoint

            if self._set_datastore_type_from_endpoint(endpoint):
                self.logger.info(f"Detected datastore from config file: {self.datastore_type}")
                return True

            return False
        except Exception as e:
            self.logger.debug(f"Error detecting from config file: {e}")
            return False

    def _detect_from_filesystem(self) -> bool:
        """
        Try to detect datastore type from filesystem evidence.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        try:
            # Check for SQLite database file
            sqlite_db_path = "/var/lib/rancher/k3s/server/db/state.db"
            if os.path.exists(sqlite_db_path):
                self.datastore_type = "sqlite"
                self.datastore_endpoint = f"sqlite://{sqlite_db_path}"
                self.logger.info("Detected SQLite database file, indicating single-server setup")
                return True

            # Check for etcd directory
            etcd_dir = "/var/lib/rancher/k3s/server/db/etcd"
            if os.path.exists(etcd_dir) and os.path.isdir(etcd_dir) and len(os.listdir(etcd_dir)) > 0:
                self.datastore_type = "etcd"
                self.is_etcd_embedded = True
                self.has_embedded_etcd = True
                self.is_ha = True
                self.logger.info("Detected embedded etcd directory, indicating HA setup")
                return True

            return False
        except Exception as e:
            self.logger.debug(f"Error detecting from filesystem: {e}")
            return False

    def _detect_from_service_file(self) -> bool:
        """
        Try to detect datastore type from K3s service file and environment.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        if not os.path.exists(self.K3S_SERVICE_FILE):
            return False

        try:
            with open(self.K3S_SERVICE_FILE, 'r') as f:
                service_content = f.read()

            # Look for environment file reference
            env_match = re.search(r'EnvironmentFile=-(.+)', service_content)
            if not env_match:
                return False

            env_file = env_match.group(1).strip()
            if not os.path.exists(env_file):
                return False

            with open(env_file, 'r') as env_f:
                env_content = env_f.read()

            # Look for datastore-endpoint in environment variables
            endpoint_match = re.search(r'K3S_DATASTORE_ENDPOINT=["\']?([^"\']+)["\']?', env_content)
            if not endpoint_match:
                return False

            endpoint = endpoint_match.group(1).strip()
            self.datastore_endpoint = endpoint

            if self._set_datastore_type_from_endpoint(endpoint):
                self.logger.info(f"Detected datastore from service env file: {self.datastore_type}")
                return True

            return False
        except Exception as e:
            self.logger.debug(f"Error detecting from service file: {e}")
            return False

    def _detect_local_server_status(self) -> bool:
        """
        Detect if the current node is running as a K3s server.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        try:
            # Check if k3s server process is running
            result = subprocess.run(
                ["pgrep", "-f", "k3s server"],
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                self.is_server_node = True
                self.logger.info("Current node is running as a K3s server")

                # Get the process command line to extract arguments
                pid = result.stdout.strip().split("\n")[0]
                if pid:
                    cmdline_path = f"/proc/{pid}/cmdline"
                    if os.path.exists(cmdline_path):
                        with open(cmdline_path, 'rb') as f:
                            cmdline = f.read().replace(b'\0', b' ').decode('utf-8', errors='replace')

                            # Look for datastore-related arguments
                            if "--datastore-endpoint" in cmdline:
                                match = re.search(r'--datastore-endpoint[= ]([^\s]+)', cmdline)
                                if match:
                                    self.datastore_endpoint = match.group(1)
                                    self.logger.info(f"Detected datastore endpoint: {self.datastore_endpoint}")

                                    if "sqlite" in self.datastore_endpoint:
                                        self.datastore_type = "sqlite"
                                    elif "etcd" in self.datastore_endpoint:
                                        self.datastore_type = "etcd"
                                        self.is_ha = True
                                        self.is_etcd_embedded = False
                                    elif any(db in self.datastore_endpoint for db in ["mysql", "postgres", "postgresql"]):
                                        self.datastore_type = "external"
                                        self.external_db_type = next((db for db in ["mysql", "postgres", "postgresql"]
                                                                   if db in self.datastore_endpoint), None)
                                        self.is_ha = True

                            # Check for cluster-init flag which indicates first server in HA cluster
                            if "--cluster-init" in cmdline:
                                self.is_ha = True
                                self.logger.info("Detected --cluster-init flag, indicating HA setup")

                            # Check for embedded etcd
                            if "--no-embed-etcd" not in cmdline and self.is_ha:
                                self.is_etcd_embedded = True
                                self.logger.info("K3s appears to be using embedded etcd")
            else:
                self.is_server_node = False
                self.logger.info("Current node is not running as a K3s server")

            return True

        except Exception as e:
            self.logger.error(f"Failed to detect local server status: {e}")
            self.detection_errors.append(f"Local server detection failed: {str(e)}")
            return False

    def _detect_node_roles(self) -> bool:
        """
        Detect control plane and worker nodes based on Kubernetes node labels.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        try:
            nodes = self.k8s_api.list_node()
            self.logger.info(f"Examining {len(nodes.items)} nodes for role detection")

            for node in nodes.items:
                node_name = node.metadata.name
                node_labels = node.metadata.labels or {}

                # Check for control plane node using control plane selectors
                is_control_plane = False
                for selector in self.labels.get_control_plane_selectors():
                    key, value = selector.split('=')
                    if node_labels.get(key) == value:
                        is_control_plane = True
                        break

                if is_control_plane:
                    self.control_plane_nodes.append(node_name)
                    self.logger.debug(f"Node {node_name} identified as control plane")
                else:
                    self.worker_nodes.append(node_name)
                    self.logger.debug(f"Node {node_name} identified as worker")

            self.logger.info(f"Detected {len(self.control_plane_nodes)} control plane nodes and {len(self.worker_nodes)} worker nodes")

            # Having multiple control plane nodes is a strong indicator of HA setup
            if len(self.control_plane_nodes) > 1:
                self.is_ha = True
                self.logger.info("Multiple control plane nodes detected, indicating HA setup")

            return True

        except Exception as e:
            self.logger.error(f"Failed to detect node roles: {e}")
            self.detection_errors.append(f"Node role detection failed: {str(e)}")
            return False

    def _determine_architecture_type(self) -> None:
        """
        Determine the final architecture type based on all collected evidence.
        """
        # If we have obvious HA indicators, architecture is high-availability
        if self.is_ha or len(self.control_plane_nodes) > 1 or self.datastore_type in ["etcd", "external"]:
            self.architecture_type = "high-availability"
            self.is_ha = True
        # Otherwise, it's a single-server setup
        elif len(self.control_plane_nodes) == 1 or self.datastore_type == "sqlite":
            self.architecture_type = "single-server"
            self.is_ha = False
        # If we're still uncertain, assume high-availability to be safe
        else:
            self.architecture_type = "high-availability"
            self.is_ha = True
            self.logger.warning("Could not definitively determine architecture type, assuming high-availability for safety")

    def _infer_from_node_count(self) -> bool:
        """
        Try to infer datastore type from control plane node count.

        Returns:
            bool: True if inference was made, False otherwise
        """
        try:
            # If we have multiple control plane nodes but couldn't identify the datastore,
            # it's most likely an external database
            if len(self.control_plane_nodes) > 1:
                self.datastore_type = "external"
                self.is_ha = True
                self.has_external_db = True
                self.logger.info("Multiple control plane nodes with unknown datastore, assuming external database")
                return True

            # Fall back for single-node clusters
            if len(self.control_plane_nodes) == 1:
                self.datastore_type = "sqlite"
                self.logger.info("Single control plane node with unknown datastore, assuming SQLite")
                return True

            return False
        except Exception as e:
            self.logger.debug(f"Error inferring from node count: {e}")
            return False

    def _run_kubectl_command(self, command: List[str]) -> Tuple[bool, str]:
        """
        Run a kubectl command and return the output.

        Args:
            command: List of command parts to run

        Returns:
            Tuple of (success, output)
        """
        try:
            result = subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True
            )
            return True, result.stdout
        except subprocess.CalledProcessError as e:
            return False, e.stderr
        except Exception as e:
            return False, str(e)

    def _set_datastore_type_from_endpoint(self, endpoint: str) -> bool:
        """
        Helper method to set datastore type based on endpoint URL.

        Args:
            endpoint: The datastore endpoint URL

        Returns:
            bool: True if datastore type was determined, False otherwise
        """
        if "sqlite" in endpoint:
            self.datastore_type = "sqlite"
            return True
        elif "etcd" in endpoint:
            self.datastore_type = "etcd"
            self.is_ha = True
            return True
        elif any(db in endpoint for db in ["mysql", "postgres", "postgresql"]):
            self.datastore_type = "external"
            self.external_db_type = next((db for db in ["mysql", "postgres", "postgresql"]
                                    if db in endpoint), None)
            self.is_ha = True
            self.has_external_db = True
            return True
        return False

    def detect_architecture(self) -> bool:
        """
        Main method to detect K3s architecture.

        Runs multiple detection strategies in sequence until architecture is determined.
        Falls back to safer assumptions if detection is uncertain.

        Returns:
            bool: True if detection was successful, False otherwise
        """
        self.logger.info("Detecting K3s cluster architecture...")

        try:
            # First, detect node roles to identify control-plane and worker nodes
            if self._detect_node_roles():
                self.detection_methods_used.append("node_roles")

            # Second, check if current node is a server by examining local processes
            if self._detect_local_server_status():
                self.detection_methods_used.append("local_server")

            # Third, detect datastore type by examining server arguments and config
            if self._detect_datastore_type():
                self.detection_methods_used.append("datastore_type")

            # Fourth, check for etcd-specific indicators
            if self._detect_etcd_presence():
                self.detection_methods_used.append("etcd_presence")

            # Finally, determine the architecture type based on collected evidence
            self._determine_architecture_type()

            self.detection_complete = True
            self.logger.info(f"Detected K3s architecture: {self.architecture_type}")
            self.logger.info(f"Datastore type: {self.datastore_type}")
            self.logger.info(f"Control plane nodes: {len(self.control_plane_nodes)}")

            return True

        except Exception as e:
            self.logger.error(f"Error during architecture detection: {e}")
            self.detection_errors.append(str(e))
            # Fall back to safest assumption
            self.architecture_type = "high-availability"  # Assume HA to be safe
            self.is_ha = True
            return False

    def get_architecture_details(self) -> Dict[str, Any]:
        """
        Returns a dictionary with architecture details.

        Returns:
            Dict containing architecture details
        """
        # Always ensure detection is complete before returning details
        if not self.detection_complete:
            self.detect_architecture()

        return {
            "architecture_type": self.architecture_type,
            "is_high_availability": self.is_ha,
            "control_plane_nodes": self.control_plane_nodes,
            "worker_nodes": self.worker_nodes,
            "datastore_type": self.datastore_type,
            "datastore_endpoint": self.datastore_endpoint,
            "is_etcd_embedded": self.is_etcd_embedded,
            "is_server_node": self.is_server_node,
            "detection_methods": self.detection_methods_used,
            "detection_errors": self.detection_errors,
            "has_embedded_etcd": self.has_embedded_etcd,
            "has_external_db": self.has_external_db,
            "external_db_type": self.external_db_type
        }

    def get_cluster_type(self) -> str:
        """
        Returns a string describing the cluster type.

        Returns:
            str: 'single-server' or 'high-availability'
        """
        if not self.detection_complete:
            self.detect_architecture()
        return self.architecture_type

    def get_required_monitoring_components(self) -> List[str]:
        """
        Returns a list of components that should be monitored based on architecture.

        Returns:
            List[str]: Component names to be monitored
        """
        base_components = ["k3s", "containerd", "kubelet", "coredns"]

        if self.is_ha:
            if self.is_etcd_embedded:
                base_components.extend(["etcd", "kube-controller-manager", "kube-scheduler", "kube-apiserver"])
            else:
                base_components.extend(["kube-controller-manager", "kube-scheduler", "kube-apiserver"])

                if self.external_db_type:
                    base_components.append(f"external-{self.external_db_type}")

        return base_components

    def is_high_availability(self) -> bool:
        """
        Returns True if cluster is running in HA mode.

        Returns:
            bool: True if the cluster is in HA mode, False otherwise
        """
        if not self.detection_complete:
            self.detect_architecture()
        return self.is_ha


class K3sLabels:
    """
    Helper class for Kubernetes label selectors and patterns with extensibility support.
    Can be configured with environment-specific label sets and custom extensions.
    """

    def __init__(self, **kwargs):
        """
        Initialize with standard K3s labels and optional custom labels.

        Args:
            **kwargs: Configuration options including:
                - k8s_api: Kubernetes API client (CoreV1Api)
                - custom_labels (dict, optional): Dictionary of custom labels to include
        """
        # Store the k8s_api client
        self.k8s_api = kwargs.get('k8s_api')

        # Core component labels (standardized)
        self.component_labels = {
            "APP_KUBERNETES_COMPONENT": "app.kubernetes.io/component",
            "APP_KUBERNETES_INSTANCE": "app.kubernetes.io/instance",
            "APP_KUBERNETES_NAME": "app.kubernetes.io/name",
            "APP_KUBERNETES_PART_OF": "app.kubernetes.io/part-of",
            "APP": "app",
            "APP_LABEL": "app.label",
            "COMPONENT": "component",
            "K8S_APP": "k8s-app",
            "NAME": "name",
            "TIER": "tier"
        }

        # Node role labels
        self.node_role_labels = {
            "NODE_ROLE_CONTROL_PLANE": "node-role.kubernetes.io/control-plane",
            "NODE_ROLE_MASTER": "node-role.kubernetes.io/master",
            "NODE_ROLE_WORKER": "node-role.kubernetes.io/worker",
            "NODE_ROLE_SERVER": "node-role.kubernetes.io/server"
        }

        # Merge custom labels if provided
        custom_labels = kwargs.get('custom_labels')
        if custom_labels:
            self.component_labels.update(custom_labels.get("component_labels", {}))
            self.node_role_labels.update(custom_labels.get("node_role_labels", {}))

    def find_component_pods(self, namespace, component_name):
        """Find pods for a specific component using multiple label patterns"""
        all_pods = []

        try:
            if self.k8s_api is None:
                return []

            # Try each label selector pattern
            for label_key in ["APP_KUBERNETES_COMPONENT", "APP_KUBERNETES_NAME",
                            "APP_KUBERNETES_INSTANCE", "K8S_APP", "APP", "COMPONENT"]:
                if label_key in self.component_labels:
                    label_selector = f"{self.component_labels[label_key]}={component_name}"
                    pods = self.k8s_api.list_namespaced_pod(
                        namespace=namespace,
                        label_selector=label_selector
                    )
                    if pods.items:
                        all_pods.extend(pods.items)

            # If no pods found by exact label match, try substring matching on names
            if not all_pods:
                pods = self.k8s_api.list_namespaced_pod(namespace=namespace)
                for pod in pods.items:
                    if component_name.lower() in pod.metadata.name.lower():
                        all_pods.append(pod)

            return all_pods
        except Exception as e:
            print(f"Error finding pods for component {component_name}: {e}")
            return []

    def get_label(self, label_key):
        """
        Get the actual label string from the label key constant.

        Args:
            label_key: The constant name for the label

        Returns:
            String with the actual label, or None if not found
        """
        if label_key in self.component_labels:
            return self.component_labels[label_key]
        elif label_key in self.node_role_labels:
            return self.node_role_labels[label_key]
        return None

    def get_label_selector(self, key, value):
        """
        Create a label selector string for use with kubectl.

        Args:
            key: Label key (either the constant name or actual label)
            value: Label value

        Returns:
            String formatted for kubectl label selector
        """
        # Check if key is a constant name, and resolve it if so
        actual_key = self.get_label(key) if key in self.component_labels or key in self.node_role_labels else key
        return f"{actual_key}={value}"

    def get_component_selector(self, component_name):
        """
        Create a selector for common component patterns.

        Args:
            component_name: Name of the component

        Returns:
            List of possible label selectors for the component
        """
        selectors = []

        # Try all relevant component label formats
        for label_key in ["APP_KUBERNETES_COMPONENT", "APP_KUBERNETES_NAME",
                          "APP_KUBERNETES_INSTANCE", "K8S_APP", "APP", "COMPONENT"]:
            if label_key in self.component_labels:
                selectors.append(self.get_label_selector(label_key, component_name))

        return selectors

    def get_control_plane_selectors(self):
        """
        Get selectors to identify control plane nodes.

        Returns:
            List of label selectors for control plane nodes
        """
        selectors = []
        for role_key in ["NODE_ROLE_CONTROL_PLANE", "NODE_ROLE_MASTER", "NODE_ROLE_SERVER"]:
            if role_key in self.node_role_labels:
                selectors.append(f"{self.node_role_labels[role_key]}=true")
        return selectors

    def get_first_pod_name(self, namespace, component_name):
        """Get the name of the first pod for a component"""
        pod_names = self.get_pod_names(namespace, component_name)
        if pod_names:
            return pod_names[0]
        return None

    def get_pod_names(self, namespace, component_name):
        """Get names of pods for a component"""
        pods = self.find_component_pods(namespace, component_name)
        return [pod.metadata.name for pod in pods]

    def register_custom_label(self, category, name, value):
        """
        Register a custom label for use in selectors.

        Args:
            category: Either 'component' or 'node_role'
            name: Constant name for the label
            value: Actual label string

        Returns:
            True if added, False if category invalid
        """
        if category == 'component':
            self.component_labels[name] = value
            return True
        elif category == 'node_role':
            self.node_role_labels[name] = value
            return True
        return False


class K3sMonitor:
    """
    K3s Cluster Monitor with Architecture Detection.

    An enhanced monitoring tool that automatically detects the K3s cluster architecture
    (single-server or high-availability) and adjusts its monitoring approach accordingly.
    """

    def __init__(self, **kwargs):
        """
        Initialize the K3s monitoring tool.

        Args:
            **kwargs: Configuration parameters including:
                config: K3sMonitorConfig instance (optional)
                Other configuration parameters used if config is not provided
        """
        # Set up configuration
        self.config = kwargs.get('config') or K3sMonitorConfig(**kwargs)

        # Set up logging
        self.logger = self._setup_logging()
        self.logger.info("Initializing K3s Monitor...")

        # Initialize state attributes
        self.is_ha_cluster = False
        self.cluster_type = "unknown"
        self.architecture_details = {}
        self.architecture_detector = None
        self.labels = None
        self.component_namespaces = {}
        self.node_metrics = []
        self.pod_metrics = []
        self.component_metrics = {}
        self.log_collection_times = []

        # Add properties to match config values for easier reference
        self.deploy_log_dir = self.config.deploy_log_dir
        self.service_logs_dir = self.config.service_logs_dir
        self.default_namespace = self.config.default_namespace
        self.log_dir = self.config.log_dir
        self.deployment_id = self.config.deployment_id
        self.duration_seconds = self.config.duration_seconds
        self.interval_seconds = self.config.interval_seconds
        self.log_max_size = self.config.log_max_size

        # Initialize Kubernetes clients
        try:
            self.logger.info("Initializing Kubernetes client...")
            k8s_config.load_kube_config()  # Using the aliased import
            self.k8s_api = client.CoreV1Api()
            self.k8s_apps_api = client.AppsV1Api()
            self.k8s_custom_api = client.CustomObjectsApi()
            self.logger.info("Successfully initialized Kubernetes client")

            # Initialize the labels handler with K8s API client
            self.labels = K3sLabels(k8s_api=self.k8s_api, custom_labels=self.config.custom_labels)

            # Detect architecture if enabled
            if self.config.detect_architecture:
                self._detect_architecture()

            # Discover components after architecture detection
            self.discover_component_namespaces()

        except Exception as e:
            self.logger.error(f"Failed to initialize K3s Monitor: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise

    def _adjust_monitoring_for_architecture(self):
        """
        Adjust monitoring components and strategies based on detected architecture.
        """
        self.logger.info("Adjusting monitoring for detected architecture...")

        # Check if architecture_detector is initialized
        if self.architecture_detector is None:
            self.logger.warning("Architecture detector is not initialized, skipping adjustments")
            return

        # Get the recommended components to monitor based on architecture
        recommended_components = self.architecture_detector.get_required_monitoring_components()
        self.logger.info(f"Recommended components to monitor: {', '.join(recommended_components)}")

        # Update log services based on architecture
        self.config.log_services = ["k3s", "kubelet", "containerd"]

        # For HA clusters, add additional services to monitor
        if self.is_ha_cluster:
            # If this is a server node, monitor etcd if it's embedded
            if self.architecture_details.get("is_server_node", False):
                if self.architecture_details.get("is_etcd_embedded", False):
                    self.config.log_services.append("etcd")

            # Add controller services
            self.config.log_services.extend(["kube-controller-manager", "kube-scheduler", "kube-apiserver"])

            # Update architecture-specific components dict
            ha_components = {
                "etcd": ["etcd"],
                "kube-apiserver": ["kube-apiserver"],
                "kube-controller-manager": ["kube-controller-manager"],
                "kube-scheduler": ["kube-scheduler"]
            }

            # For HA, make sure we're monitoring etcd and control plane components
            self.config.components.update(ha_components)
            self.logger.info("Added HA-specific components to monitoring")

        self.logger.info(f"Adjusted services to monitor: {', '.join(self.config.log_services)}")

    def _detect_architecture(self):
        """Detect K3s architecture and adjust monitoring configuration accordingly."""
        try:
            self.logger.info("Initializing K3s Architecture Detector...")
            self.architecture_detector = K3sArchitectureDetector(
                k8s_api=self.k8s_api,
                k3s_labels=self.labels,
                logger=self.logger
            )

            if not self.architecture_detector.detect_architecture():
                self.logger.warning("Architecture detection returned false, using fallback configuration")
                self.is_ha_cluster = False
                self.cluster_type = "unknown"
                self.architecture_details = {}
                return

            self.architecture_details = self.architecture_detector.get_architecture_details()
            self.is_ha_cluster = self.architecture_detector.is_high_availability()
            self.cluster_type = self.architecture_detector.get_cluster_type()

            self.logger.info(f"Detected K3s cluster type: {self.cluster_type}")
            self.logger.info(f"High Availability mode: {self.is_ha_cluster}")

            # Adjust monitoring based on architecture
            self._adjust_monitoring_for_architecture()
        except Exception as e:
            self.logger.error(f"Architecture detection failed: {e}")
            self.logger.warning("Falling back to default monitoring configuration")
            self.is_ha_cluster = False
            self.cluster_type = "unknown"
            self.architecture_details = {}
            self.architecture_detector = None

    def _extract_metrics_from_summary(self, summary_file: Path) -> Dict[str, Any]:
        """
        Extract key metrics from a summary file.

        Args:
            summary_file: Path to summary file

        Returns:
            Dictionary of extracted metrics
        """
        metrics = {}

        try:
            with open(summary_file, 'r') as f:
                content = f.read()

                # Extract node metrics
                node_count_match = re.search(r'Total nodes: (\d+)', content)
                if node_count_match:
                    metrics['node_count'] = int(node_count_match.group(1))

                ready_nodes_match = re.search(r'Ready nodes: (\d+)/(\d+)', content)
                if ready_nodes_match:
                    metrics['ready_nodes'] = int(ready_nodes_match.group(1))

                # Extract pod metrics
                pod_count_match = re.search(r'Total pods: (\d+)', content)
                if pod_count_match:
                    metrics['pod_count'] = int(pod_count_match.group(1))

                running_pods_match = re.search(r'Running pods: (\d+)/(\d+)', content)
                if running_pods_match:
                    metrics['running_pods'] = int(running_pods_match.group(1))

                ready_pods_match = re.search(r'Ready pods: (\d+)/(\d+)', content)
                if ready_pods_match:
                    metrics['ready_pods'] = int(ready_pods_match.group(1))

                # Extract component metrics
                component_count_match = re.search(r'Monitored components: (\d+)', content)
                if component_count_match:
                    metrics['component_count'] = int(component_count_match.group(1))

                # Count component statuses
                healthy_components = len(re.findall(r'Status: Healthy', content))
                degraded_components = len(re.findall(r'Status: Degraded', content))
                unhealthy_components = len(re.findall(r'Status: Unhealthy', content))

                metrics['healthy_components'] = healthy_components
                metrics['degraded_components'] = degraded_components
                metrics['unhealthy_components'] = unhealthy_components

        except Exception as e:
            self.logger.error(f"Failed to extract metrics from summary file: {e}")

        return metrics

    def _extract_nodes_from_metrics(self, metrics_file: Path) -> set:
        """
        Extract node names from a node metrics file.

        Args:
            metrics_file: Path to node metrics file

        Returns:
            Set of node names
        """
        nodes = set()

        try:
            with open(metrics_file, 'r') as f:
                content = f.read()

                # Find all node names using regex
                node_matches = re.findall(r'Node: ([^\n]+)', content)
                nodes.update(node_matches)

        except Exception as e:
            self.logger.error(f"Failed to extract nodes from metrics file: {e}")

        return nodes

    def _init_components(self):
        """Initialize monitoring components with improved error handling."""
        try:
            # Initialize Kubernetes client
            self.logger.info("Initializing Kubernetes client...")
            k8s_config.load_kube_config()  # Using the aliased import
            self.k8s_api = client.CoreV1Api()
            self.k8s_apps_api = client.AppsV1Api()
            self.k8s_custom_api = client.CustomObjectsApi()

            # Initialize the label helper
            self.labels = K3sLabels(k8s_api=self.k8s_api, custom_labels=self.config.custom_labels)

            # Add additional components
            # Use config's components rather than undefined self.components
            self.config.components.update(self.config.additional_components)

            # Initialize component namespaces dict
            self.component_namespaces = {}

            # Initialize architecture detector if enabled
            if self.config.detect_architecture:
                self.logger.info("Initializing K3s Architecture Detector...")
                try:
                    self.architecture_detector = K3sArchitectureDetector(
                        k8s_api=self.k8s_api,
                        k3s_labels=self.labels,
                        logger=self.logger
                    )

                    if not self.architecture_detector.detect_architecture():
                        self.logger.warning("Architecture detection returned false, using fallback configuration")
                        self.is_ha_cluster = False
                        self.cluster_type = "unknown"
                        self.architecture_details = {}
                        return

                    self.architecture_details = self.architecture_detector.get_architecture_details()
                    self.is_ha_cluster = self.architecture_detector.is_high_availability()
                    self.cluster_type = self.architecture_detector.get_cluster_type()

                    self.logger.info(f"Detected K3s cluster type: {self.cluster_type}")
                    self.logger.info(f"High Availability mode: {self.is_ha_cluster}")

                    # Adjust monitoring based on architecture
                    self._adjust_monitoring_for_architecture()
                except Exception as e:
                    self.logger.error(f"Architecture detection failed: {e}")
                    self.logger.warning("Falling back to default monitoring configuration")
                    self.is_ha_cluster = False
                    self.cluster_type = "unknown"
                    self.architecture_details = {}
                    self.architecture_detector = None
            else:
                self.is_ha_cluster = False
                self.cluster_type = "unknown"
                self.architecture_details = {}

            # Discover namespaces for components and skip components not found
            self.discover_component_namespaces()

            self.logger.info("K3s Monitor initialization complete")
        except Exception as e:
            self.logger.error(f"Failed to initialize monitoring components: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise

    def _setup_logging(self):
        """
        Set up logging with appropriate level and handlers.

        Returns:
            Logger: Configured logger instance
        """
        log_level = logging.DEBUG if self.config.verbose else logging.INFO
        log_file = self.config.deploy_log_dir / "k3s-monitor.log"

        # Configure root logger
        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        return logging.getLogger("k3s-monitor")

    def _write_metric_comparison(self, file, metric_name, prev_value, curr_value):
        """
        Write a formatted metric comparison line.

        Args:
            file: File object to write to
            metric_name: Name of the metric
            prev_value: Previous value
            curr_value: Current value
        """
        if prev_value is None or curr_value is None:
            file.write(f"  {metric_name}: {prev_value or 'N/A'} → {curr_value or 'N/A'}\n")
            return

        if prev_value == curr_value:
            file.write(f"  {metric_name}: {prev_value} (no change)\n")
        elif curr_value > prev_value:
            file.write(f"  {metric_name}: {prev_value} → {curr_value} (+{curr_value - prev_value})\n")
        else:
            file.write(f"  {metric_name}: {prev_value} → {curr_value} (-{prev_value - curr_value})\n")

    def check_prerequisites(self):
        """
        Check if all prerequisites are met to run the monitoring.

        Returns:
            bool: True if all prerequisites are met, False otherwise
        """
        try:
            # Check if journalctl is available
            subprocess.run(["journalctl", "--version"],
                          check=True, capture_output=True, text=True)

            # Check if jq is available (used in some helper functions)
            subprocess.run(["jq", "--version"],
                          check=True, capture_output=True, text=True)

            # Check if kubectl is available
            subprocess.run(["kubectl", "version", "--client"],
                          check=True, capture_output=True, text=True)

            # Check cluster connectivity
            nodes = self.k8s_api.list_node()
            self.logger.info(f"Connected to cluster with {len(nodes.items)} nodes")

            return True
        except Exception as e:
            self.logger.error(f"Prerequisite check failed: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

    def collect_cilium_metrics(self) -> None:
        """Collect Cilium networking metrics."""
        timestamp = datetime.datetime.now()
        self.logger.info("Checking for Cilium metrics...")

        try:
            # Check if Cilium is installed
            cilium_namespace = self.component_namespaces.get("cilium", self.default_namespace)

            # Get Cilium status using kubectl
            cilium_metrics_file = self.deploy_log_dir / "cilium-metrics.log"

            with open(cilium_metrics_file, 'w') as f:
                f.write(f"=== Cilium metrics collected at {timestamp} ===\n\n")

                # Find the first cilium pod by name pattern
                cilium_pod = None
                try:
                    pods = self.k8s_api.list_namespaced_pod(namespace=cilium_namespace)
                    for pod in pods.items:
                        if "cilium" in pod.metadata.name and "cilium-operator" not in pod.metadata.name:
                            cilium_pod = pod.metadata.name
                            break
                except Exception as e:
                    self.logger.error(f"Error finding Cilium pods: {e}")

                if not cilium_pod:
                    # Try using label selector
                    try:
                        success, output = self.run_kubectl_command([
                            "kubectl", "get", "pods", "-n", cilium_namespace,
                            "-l", "k8s-app=cilium", "-o", "name", "--limit=1"
                        ])

                        if success and output.strip():
                            # Extract pod name from the output (format will be "pod/name")
                            pod_name = output.strip().split('\n')[0]
                            if pod_name.startswith("pod/"):
                                cilium_pod = pod_name[4:]
                            else:
                                cilium_pod = pod_name
                    except Exception as e:
                        self.logger.error(f"Error finding Cilium pods by label: {e}")

                if not cilium_pod:
                    f.write("No Cilium pods found. Cilium might not be installed or might use different labels.\n")
                    self.logger.info("No Cilium pods found")
                    return

                # Get Cilium status
                f.write("=== Cilium status ===\n")
                success, output = self.run_kubectl_command([
                    "kubectl", "exec", "-n", cilium_namespace,
                    cilium_pod, "--", "cilium", "status"
                ])

                if success:
                    f.write(output + "\n\n")
                else:
                    f.write("Failed to get Cilium status\n\n")

                # Get Cilium endpoints
                f.write("=== Cilium endpoints ===\n")
                success, output = self.run_kubectl_command([
                    "kubectl", "exec", "-n", cilium_namespace,
                    cilium_pod, "--", "cilium", "endpoint", "list", "-o", "json"
                ])

                if success:
                    # Process and summarize endpoint data to avoid excessive output
                    try:
                        endpoints = json.loads(output)
                        f.write(f"Total endpoints: {len(endpoints)}\n")

                        # Count endpoints by state
                        endpoint_states = {}
                        for endpoint in endpoints:
                            state = endpoint.get("state", "unknown")
                            endpoint_states[state] = endpoint_states.get(state, 0) + 1

                        f.write("Endpoint states:\n")
                        for state, count in endpoint_states.items():
                            f.write(f"  {state}: {count}\n")
                    except json.JSONDecodeError:
                        f.write("Failed to parse Cilium endpoints JSON\n")
                else:
                    f.write("Failed to get Cilium endpoints\n\n")

                # Get Cilium service list
                f.write("\n=== Cilium services ===\n")
                success, output = self.run_kubectl_command([
                    "kubectl", "exec", "-n", cilium_namespace,
                    cilium_pod, "--", "cilium", "service", "list"
                ])

                if success:
                    f.write(output + "\n\n")
                else:
                    f.write("Failed to get Cilium services\n\n")

            self.logger.info("Collected Cilium metrics")

        except Exception as e:
            self.logger.error(f"Error collecting Cilium metrics: {e}")

    def collect_component_metrics(self):
        """
        Collect metrics from specific K3s components and installed applications.
        Stores metrics in self.component_metrics dictionary.
        """
        timestamp = datetime.datetime.now()
        self.logger.info("Collecting component metrics...")

        try:
            components_metrics_file = self.config.deploy_log_dir / "component-metrics.log"

            with open(components_metrics_file, 'a') as f:
                f.write(f"=== Component metrics collected at {timestamp} ===\n\n")

                # Iterate through discovered components
                for component, namespace in self.component_namespaces.items():
                    f.write(f"Component: {component} (namespace: {namespace})\n")
                    f.write("-" * 60 + "\n")
                    self.logger.info(f"Collecting metrics for component {component} in namespace {namespace}")

                    # Initialize component metrics if not exists
                    if component not in self.component_metrics:
                        self.component_metrics[component] = []

                    # Get pods for this component
                    try:
                        # Try different label selectors based on common patterns
                        component_pods = []
                        selectors = self.labels.get_component_selector(component)

                        for selector in selectors:
                            success, output = self.run_kubectl_command([
                                "kubectl", "get", "pods", "-n", namespace,
                                "-l", selector, "-o", "json"
                            ])

                            if success:
                                import json
                                pod_data = json.loads(output)
                                if pod_data["items"]:
                                    component_pods.extend(pod_data["items"])
                                    # Grammatically correct message based on pod count
                                    pod_count = len(pod_data["items"])
                                    pod_word = "pod" if pod_count == 1 else "pods"
                                    self.logger.info(f"Found {pod_count} {pod_word} for component {component} using selector {selector}")

                        # If no pods found with selectors, try name-based matching
                        if not component_pods:
                            success, output = self.run_kubectl_command([
                                "kubectl", "get", "pods", "-n", namespace, "-o", "json"
                            ])

                            if success:
                                import json
                                all_pods = json.loads(output)
                                for pod in all_pods["items"]:
                                    pod_name = pod["metadata"]["name"]
                                    if any(prefix in pod_name.lower() for prefix in self.config.components.get(component, [])):
                                        component_pods.append(pod)

                        if component_pods:
                            # Grammatically correct message in the file
                            pod_count = len(component_pods)
                            pod_word = "pod" if pod_count == 1 else "pods"
                            f.write(f"Found {pod_count} {pod_word} for component {component}\n")

                            # Collect metrics for this component
                            component_metric = {
                                "timestamp": timestamp,
                                "name": component,
                                "namespace": namespace,
                                "pod_count": pod_count,
                                "healthy_pods": 0,
                                "status": "Healthy"  # Default status
                            }

                            # Check health of pods
                            for pod in component_pods:
                                pod_name = pod["metadata"]["name"]
                                pod_status = pod["status"]["phase"]

                                f.write(f"  Pod: {pod_name}\n")
                                f.write(f"    Status: {pod_status}\n")

                                # Check container readiness
                                is_ready = False
                                if "containerStatuses" in pod["status"]:
                                    is_ready = all(container["ready"] for container in pod["status"]["containerStatuses"])
                                    restart_count = sum(container["restartCount"] for container in pod["status"]["containerStatuses"])
                                    f.write(f"    Ready: {is_ready}\n")
                                    f.write(f"    Restarts: {restart_count}\n")

                                # Get resource usage if possible
                                success, output = self.run_kubectl_command([
                                    "kubectl", "top", "pod", pod_name, "-n", namespace
                                ])

                                if success:
                                    f.write(f"    Resource Usage:\n")
                                    for line in output.splitlines():
                                        if pod_name in line:
                                            f.write(f"      {line}\n")

                                # Increment healthy pod count if running and ready
                                if pod_status == "Running" and is_ready:
                                    component_metric["healthy_pods"] += 1

                            # Determine component status
                            if component_metric["healthy_pods"] == 0:
                                component_metric["status"] = "Unhealthy"
                            elif component_metric["healthy_pods"] < component_metric["pod_count"]:
                                component_metric["status"] = "Degraded"

                            f.write(f"  Overall Status: {component_metric['status']}\n")

                            # Grammatically correct message for healthy pods
                            healthy_count = component_metric['healthy_pods']
                            total_count = component_metric['pod_count']
                            pod_word = "pod" if total_count == 1 else "pods"
                            f.write(f"  Healthy Pods: {healthy_count}/{total_count} {pod_word}\n\n")

                            # Add component metrics to collection
                            self.component_metrics[component].append(component_metric)
                        else:
                            f.write(f"No pods found for component {component}\n\n")

                    except Exception as e:
                        self.logger.error(f"Failed to collect metrics for component {component}: {e}")
                        f.write(f"Error collecting metrics: {str(e)}\n\n")
                        import traceback
                        self.logger.error(traceback.format_exc())

            self.logger.info(f"Collected metrics for {len(self.component_namespaces)} components")
        except Exception as e:
            self.logger.error(f"Failed to collect component metrics: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def collect_etcd_metrics(self) -> None:
        """Collect etcd metrics for high-availability K3s clusters using kubectl."""
        # Skip for single-server setups
        if not getattr(self, 'is_ha_cluster', False):
            self.logger.info("Skipping etcd metrics collection for single-server setup")
            return

        timestamp = datetime.datetime.now()
        self.logger.info("Collecting etcd metrics")

        try:
            # First check for the existence of etcd endpoints, which indicates the cluster is using etcd
            success, output = self.run_kubectl_command([
                "kubectl", "get", "endpoints", "kubernetes", "-n", "default", "-o", "json"
            ])

            if not success:
                self.logger.error("Failed to check if cluster is using etcd")
                return

            # Parse the endpoints to check if etcd is being used
            try:
                import json
                endpoints = json.loads(output)
                # If we have endpoints, the cluster is likely using etcd in HA mode
                using_etcd = len(endpoints.get("subsets", [])) > 0

                if not using_etcd and not self.architecture_details.get('is_etcd_embedded', False):
                    self.logger.info("Cluster does not appear to be using etcd")
                    return

                self.logger.info("Cluster is using etcd in HA mode")

                # Create etcd metrics file
                etcd_metrics_file = self.deploy_log_dir / "etcd-metrics.log"

                with open(etcd_metrics_file, 'w') as f:
                    f.write(f"=== etcd metrics collected at {timestamp} ===\n\n")
                    f.write(f"Cluster type: {self.cluster_type}\n")
                    f.write(f"Datastore type: {self.architecture_details.get('datastore_type', 'Unknown')}\n")

                    if self.architecture_details.get('is_etcd_embedded', False):
                        f.write("Using embedded etcd\n\n")
                    else:
                        f.write("Using external etcd\n\n")

                    # Get the control plane nodes
                    control_plane_selector = self.labels.get_control_plane_selectors()[0]  # Use first selector
                    success, output = self.run_kubectl_command([
                        "kubectl", "get", "nodes",
                        "-l", control_plane_selector,
                        "-o", "jsonpath='{.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'"
                    ])

                    if not success or not output:
                        self.logger.warning("Could not find control plane nodes")
                    else:
                        # List of K3s control plane nodes
                        control_plane_ips = output.strip("'").split()
                        self.logger.info(f"Found {len(control_plane_ips)} control plane nodes")
                        f.write(f"Control plane nodes: {', '.join(control_plane_ips)}\n\n")

                    # Get K3s server version (includes etcd version)
                    f.write("=== K3s server version ===\n")
                    success, output = self.run_kubectl_command(["kubectl", "version"])

                    if success:
                        f.write(output + "\n\n")
                    else:
                        f.write("Failed to get K3s version\n\n")

                    # Get etcd metrics from Kubernetes API
                    f.write("\n=== etcd metrics from Kubernetes API ===\n")
                    success, output = self.run_kubectl_command([
                        "kubectl", "get", "--raw", "/metrics"
                    ])

                    if success:
                        # Extract etcd-related metrics from the output
                        etcd_metrics = []
                        for line in output.splitlines():
                            if "etcd_" in line and not line.startswith("#"):
                                etcd_metrics.append(line)

                        if etcd_metrics:
                            f.write("\nFound etcd metrics in Kubernetes API:\n")
                            for metric in etcd_metrics[:50]:  # Increased limit to 50 metrics
                                f.write(metric + "\n")

                            if len(etcd_metrics) > 50:
                                f.write(f"... and {len(etcd_metrics) - 50} more metrics\n")
                        else:
                            f.write("No etcd metrics found in Kubernetes API metrics\n")
                    else:
                        f.write("Failed to get metrics from Kubernetes API\n")

                    # Try to get etcd health using kubectl
                    f.write("\n=== etcd health status ===\n")

                    # Find etcd pods using label selectors
                    success, output = self.run_kubectl_command([
                        "kubectl", "get", "pods", "-n", "kube-system",
                        "-l", self.labels.get_label_selector("COMPONENT", "etcd"),
                        "-o", "name"
                    ])

                    if success and output.strip():
                        etcd_pods = output.strip().split('\n')
                        if etcd_pods and len(etcd_pods) > 0:
                            etcd_pod = etcd_pods[0]  # Use the first etcd pod

                            # Extract just the pod name without the "pod/" prefix
                            if etcd_pod.startswith("pod/"):
                                etcd_pod_name = etcd_pod[4:]
                            else:
                                etcd_pod_name = etcd_pod

                            # Try to get etcd health status
                            success, output = self.run_kubectl_command([
                                "kubectl", "exec", "-n", "kube-system", etcd_pod_name, "--",
                                "etcdctl", "endpoint", "health", "--cluster"
                            ])

                            if success:
                                f.write("etcd cluster health status:\n")
                                f.write(output + "\n")
                            else:
                                # Try alternative approaches for K3s
                                f.write("Could not get etcd health directly, trying alternatives for K3s\n")

                                # Try K3s specific approach - checking through k3s
                                success, output = self.run_kubectl_command([
                                    "kubectl", "get", "nodes",
                                    "-l", control_plane_selector,
                                    "-o", "custom-columns=NAME:.metadata.name"
                                ])

                                if success and output.strip():
                                    header_and_nodes = output.strip().split('\n')
                                    # Skip the header row
                                    control_plane_nodes = header_and_nodes[1:] if len(header_and_nodes) > 1 else []

                                    if control_plane_nodes:
                                        f.write(f"Found control plane nodes: {', '.join(control_plane_nodes)}\n")
                                        f.write("K3s clusters typically embed etcd within the k3s process\n")

                                        # Get K3s process status on one of the control plane nodes
                                        success, output = self.run_kubectl_command([
                                            "kubectl", "get", "--raw",
                                            "/api/v1/namespaces/kube-system/pods"
                                        ])

                                        if success:
                                            f.write("Control plane pods in kube-system namespace are running\n")
                                            f.write("This indicates that etcd (embedded in K3s) is likely functioning\n")
                                        else:
                                            f.write("Could not verify control plane pod status\n")
                                    else:
                                        f.write("No control plane nodes found\n")
                                else:
                                    f.write("Could not identify control plane nodes\n")
                        else:
                            f.write("No dedicated etcd pods found. K3s likely uses embedded etcd.\n")
                            # Try to check K3s system pod status as an indicator
                            success, output = self.run_kubectl_command([
                                "kubectl", "get", "pods", "-n", "kube-system",
                                "-l", self.labels.get_label_selector("K8S_APP", "kube-controller-manager")
                            ])

                            if success:
                                f.write("Controller manager is running, indicating etcd is functional:\n")
                                f.write(output + "\n")
                            else:
                                f.write("Could not verify controller manager status\n")
                    else:
                        f.write("No dedicated etcd pods found. K3s uses embedded etcd.\n")
                        # Check k3s server components
                        success, output = self.run_kubectl_command([
                            "kubectl", "get", "pods", "-n", "kube-system",
                            "-o", "wide"
                        ])

                        if success:
                            f.write("K3s system pods (which include embedded etcd):\n")
                            # Filter output to show only likely control plane pods
                            system_pods = []
                            for line in output.strip().split('\n'):
                                if any(x in line.lower() for x in ["server", "control", "master", "k3s"]):
                                    system_pods.append(line)

                            if system_pods:
                                for pod in system_pods:
                                    f.write(pod + "\n")
                            else:
                                f.write("No specific K3s server pods identified\n")
                        else:
                            f.write("Could not list K3s system pods\n")

                self.logger.info(f"Collected etcd metrics to {etcd_metrics_file}")

            except json.JSONDecodeError:
                self.logger.error("Failed to parse endpoints JSON")
                return

        except Exception as e:
            self.logger.error(f"Error collecting etcd metrics: {e}")

    def collect_kubernetes_logs(self) -> None:
        """
        Collect logs from Kubernetes system pods.
        Stores logs in the service logs directory.
        """
        timestamp = datetime.datetime.now()
        self.logger.info("Collecting Kubernetes logs...")

        try:
            # Create a directory for k8s logs
            k8s_logs_dir = self.service_logs_dir / "k8s"
            k8s_logs_dir.mkdir(exist_ok=True)

            # Identify core system pods for logging
            system_components = {
                "coredns": ["coredns", "kube-dns"],
                "kube-proxy": ["kube-proxy"],
                "metrics-server": ["metrics-server"]
            }

            # Add control plane components for HA clusters
            if getattr(self, 'is_ha_cluster', False):
                system_components.update({
                    "kube-controller-manager": ["kube-controller-manager"],
                    "kube-scheduler": ["kube-scheduler"],
                    "kube-apiserver": ["kube-apiserver"],
                    "etcd": ["etcd"]
                })

            # Get logs for each system component
            for component, prefixes in system_components.items():
                # Try to find pods for this component by name pattern
                component_pods = []

                # Get all pods in the default namespace
                try:
                    pods = self.k8s_api.list_namespaced_pod(namespace=self.default_namespace)
                    for pod in pods.items:
                        pod_name = pod.metadata.name
                        if any(prefix in pod_name for prefix in prefixes):
                            component_pods.append(pod_name)
                except Exception as e:
                    self.logger.error(f"Failed to list pods for component {component}: {e}")

                # If no pods found by name pattern, try label selectors
                if not component_pods:
                    # Try different label selectors
                    for label_key in ["k8s-app", "app", "component"]:
                        for prefix in prefixes:
                            try:
                                pod_name = self.get_pod_by_label(
                                    self.default_namespace,
                                    f"{label_key}={prefix}"
                                )
                                if pod_name:
                                    component_pods.append(pod_name)
                                    break
                            except Exception as e:
                                self.logger.debug(f"Error finding pods with label {label_key}={prefix}: {e}")

                        if component_pods:
                            break

                if component_pods:
                    self.logger.debug(f"Found {len(component_pods)} pods for component {component}")

                    # Get logs for the first pod only (to avoid excessive logs)
                    pod_name = component_pods[0]
                    log_file = k8s_logs_dir / f"{component}-{timestamp.strftime('%Y%m%d-%H%M%S')}.log"

                    try:
                        # Use kubectl logs to get pod logs
                        success, output = self.run_kubectl_command([
                            "kubectl", "logs", pod_name, "-n", self.default_namespace,
                            "--tail=100"  # Limit to last 100 lines
                        ])

                        if success:
                            with open(log_file, 'w') as f:
                                f.write(f"=== Logs for {component} pod {pod_name} ===\n\n")
                                f.write(output)
                            self.logger.debug(f"Collected logs for component {component}")
                        else:
                            with open(log_file, 'w') as f:
                                f.write(f"Failed to collect logs for component {component} pod {pod_name}\n")
                                f.write(f"Error: {output}\n")

                    except Exception as e:
                        self.logger.error(f"Failed to collect logs for component {component}: {e}")
                        with open(log_file, 'w') as f:
                            f.write(f"Failed to collect logs for component {component}: {str(e)}\n")
                else:
                    self.logger.debug(f"No pods found for component {component}")

            self.logger.info("Collected Kubernetes logs for system components")
        except Exception as e:
            self.logger.error(f"Failed to collect Kubernetes logs: {e}")

    def collect_node_metrics(self):
        """
        Collect metrics from all nodes in the cluster.
        Stores metrics in self.node_metrics list.
        """
        timestamp = datetime.datetime.now()
        self.logger.info("Collecting node metrics...")

        try:
            # Get all nodes
            nodes = self.k8s_api.list_node()
            node_count = len(nodes.items)
            node_word = "node" if node_count == 1 else "nodes"
            node_metrics_file = self.config.deploy_log_dir / "node-metrics.log"

            with open(node_metrics_file, 'a') as f:
                f.write(f"=== Node metrics collected at {timestamp} ===\n\n")

                for node in nodes.items:
                    node_name = node.metadata.name
                    node_status = node.status

                    # Get node conditions
                    conditions = {cond.type: cond.status for cond in node_status.conditions}

                    # Get node resources
                    allocatable = node_status.allocatable
                    capacity = node_status.capacity

                    # Create metrics entry
                    metrics = {
                        "timestamp": timestamp,
                        "name": node_name,
                        "conditions": conditions,
                        "allocatable": allocatable,
                        "capacity": capacity,
                    }

                    # Add to metrics collection
                    self.node_metrics.append(metrics)

                    # Write to log file
                    f.write(f"Node: {node_name}\n")
                    f.write(f"  Status: {conditions.get('Ready', 'Unknown')}\n")
                    f.write(f"  Capacity:\n")
                    f.write(f"    CPU: {capacity.get('cpu', 'Unknown')}\n")
                    f.write(f"    Memory: {capacity.get('memory', 'Unknown')}\n")
                    f.write(f"    Pods: {capacity.get('pods', 'Unknown')}\n")
                    f.write(f"  Allocatable:\n")
                    f.write(f"    CPU: {allocatable.get('cpu', 'Unknown')}\n")
                    f.write(f"    Memory: {allocatable.get('memory', 'Unknown')}\n")
                    f.write(f"    Pods: {allocatable.get('pods', 'Unknown')}\n\n")

                    # Try to get node resource usage with kubectl top
                    success, output = self.run_kubectl_command([
                        "kubectl", "top", "node", node_name
                    ])

                    if success:
                        f.write(f"  Resource Usage:\n")
                        for line in output.splitlines():
                            if node_name in line:
                                f.write(f"    {line}\n")
                    f.write("\n")

            self.logger.info(f"Collected metrics for {node_count} {node_word}")
        except Exception as e:
            self.logger.error(f"Failed to collect node metrics: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def collect_pod_metrics(self):
        """
        Collect metrics from pods in relevant namespaces.
        Stores metrics in self.pod_metrics list.
        """
        timestamp = datetime.datetime.now()
        self.logger.info("Collecting pod metrics...")

        try:
            # Get all pods across all namespaces for system monitoring
            pods = self.k8s_api.list_pod_for_all_namespaces(watch=False)
            pod_metrics_file = self.config.deploy_log_dir / "pod-metrics.log"

            with open(pod_metrics_file, 'a') as f:
                f.write(f"=== Pod metrics collected at {timestamp} ===\n\n")

                # Group pods by namespace for better reporting
                namespace_pods = defaultdict(list)
                for pod in pods.items:
                    namespace_pods[pod.metadata.namespace].append(pod)

                for namespace, ns_pods in namespace_pods.items():
                    pod_count = len(ns_pods)
                    pod_word = "pod" if pod_count == 1 else "pods"
                    f.write(f"Namespace: {namespace}\n")
                    f.write("-" * 60 + "\n")

                    # Check for k3s-related and system namespace pods
                    if namespace == self.config.default_namespace or namespace == "default":
                        f.write(f"System namespace: {namespace}\n")

                    for pod in ns_pods:
                        pod_name = pod.metadata.name
                        pod_status = pod.status

                        # Check if pod is ready
                        is_ready = False
                        if pod_status.container_statuses:
                            is_ready = all(container.ready for container in pod_status.container_statuses)

                        # Create metrics entry
                        metrics = {
                            "timestamp": timestamp,
                            "name": pod_name,
                            "namespace": namespace,
                            "phase": pod_status.phase,
                            "ready": is_ready,
                            "restart_count": sum(container.restart_count for container in pod_status.container_statuses) if pod_status.container_statuses else 0,
                            "host_ip": pod_status.host_ip,
                            "pod_ip": pod_status.pod_ip
                        }

                        # Add to metrics collection
                        self.pod_metrics.append(metrics)

                        # Write to log file
                        f.write(f"Pod: {pod_name}\n")
                        f.write(f"  Status: {pod_status.phase}\n")
                        f.write(f"  Ready: {is_ready}\n")

                        if pod_status.container_statuses:
                            container_count = len(pod_status.container_statuses)
                            container_word = "container" if container_count == 1 else "containers"
                            f.write(f"  {container_word.capitalize()}:\n")
                            for container in pod_status.container_statuses:
                                f.write(f"    {container.name}: Ready={container.ready}, Restarts={container.restart_count}\n")

                        # Try to get pod resource usage with kubectl top
                        success, output = self.run_kubectl_command([
                            "kubectl", "top", "pod", pod_name, "-n", namespace
                        ])

                        if success:
                            f.write(f"  Resource Usage:\n")
                            for line in output.splitlines():
                                if pod_name in line:
                                    f.write(f"    {line}\n")
                        f.write("\n")

                    f.write("\n")

            # Use grammatically correct logging
            pod_count = len(pods.items)
            pod_word = "pod" if pod_count == 1 else "pods"
            namespace_count = len(namespace_pods)
            namespace_word = "namespace" if namespace_count == 1 else "namespaces"
            self.logger.info(f"Collected metrics for {pod_count} {pod_word} across {namespace_count} {namespace_word}")
        except Exception as e:
            self.logger.error(f"Failed to collect pod metrics: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def collect_service_logs(self):
        """
        Collect logs from critical K3s services using journalctl.
        Stores logs in the service logs directory.
        """
        timestamp = datetime.datetime.now()
        self.logger.info("Collecting service logs...")

        try:
            # Record log collection time
            self.log_collection_times.append(timestamp)

            # Get logs for each service
            for service in self.config.log_services:
                service_log_file = self.config.service_logs_dir / f"{service}-{timestamp.strftime('%Y%m%d-%H%M%S')}.log"

                # Determine time range for log collection
                time_range = "--since='10 minutes ago'"
                if len(self.log_collection_times) > 1:
                    # Calculate time since last collection minus 10 seconds overlap
                    last_time = self.log_collection_times[-2]
                    seconds_diff = (timestamp - last_time).total_seconds() - 10
                    if seconds_diff > 0:
                        time_range = f"--since='{int(seconds_diff)} seconds ago'"

                # Use journalctl to get service logs
                command = f"journalctl -u {service}.service {time_range} > {service_log_file}"

                try:
                    subprocess.run(command, shell=True, check=True)
                    self.logger.debug(f"Collected logs for service {service}")

                    # Check log size
                    if os.path.getsize(service_log_file) == 0:
                        # No logs found, try alternative unit name formats
                        alt_services = []
                        if service == "k3s":
                            alt_services = ["k3s-server", "k3s-agent"]

                        for alt_service in alt_services:
                            alt_command = f"journalctl -u {alt_service}.service {time_range} > {service_log_file}"
                            try:
                                subprocess.run(alt_command, shell=True, check=True)
                                if os.path.getsize(service_log_file) > 0:
                                    self.logger.debug(f"Collected logs for alternative service {alt_service}")
                                    break
                            except Exception as e:
                                self.logger.debug(f"Failed to collect logs for alternative service {alt_service}: {e}")

                    # If still empty, check using process name
                    if os.path.getsize(service_log_file) == 0:
                        process_command = f"journalctl _COMM={service} {time_range} > {service_log_file}"
                        try:
                            subprocess.run(process_command, shell=True, check=True)
                            self.logger.debug(f"Collected logs for process {service}")
                        except Exception as e:
                            self.logger.debug(f"Failed to collect logs for process {service}: {e}")

                except Exception as e:
                    self.logger.error(f"Failed to collect logs for service {service}: {e}")
                    # Create an empty file to maintain consistency
                    with open(service_log_file, 'w') as f:
                        f.write(f"Failed to collect logs for service {service}: {str(e)}\n")

            self.logger.info(f"Collected logs for {len(self.config.log_services)} services")
        except Exception as e:
            self.logger.error(f"Failed to collect service logs: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def compare_with_previous(self) -> None:
        """
        Compare current monitoring results with previous runs if available.
        Creates a comparison.log file in the deployment directory.
        """
        self.logger.info("Comparing with previous monitoring results...")

        try:
            # Find previous monitoring directories
            previous_dirs = sorted([
                d for d in self.log_dir.glob("*-*")
                if d.is_dir() and d.name != self.deployment_id
            ], key=lambda d: d.name, reverse=True)

            if not previous_dirs:
                self.logger.info("No previous monitoring results found for comparison")
                return

            # Use the most recent previous directory
            prev_dir = previous_dirs[0]
            self.logger.info(f"Comparing with previous monitoring run: {prev_dir.name}")

            comparison_file = self.deploy_log_dir / "comparison.log"

            with open(comparison_file, 'w') as f:
                f.write("=== K3s Monitoring Comparison ===\n")
                f.write(f"Current deployment: {self.deployment_id}\n")
                f.write(f"Previous deployment: {prev_dir.name}\n\n")

                # Compare node metrics if available
                prev_summary_file = prev_dir / "summary.log"
                if prev_summary_file.exists():
                    f.write("=== Summary Comparison ===\n")

                    # Extract key metrics from previous summary
                    prev_metrics = self._extract_metrics_from_summary(prev_summary_file)

                    # Generate current summary metrics
                    curr_metrics = {}

                    # Node metrics
                    node_count = len(set(metric['name'] for metric in self.node_metrics)) if self.node_metrics else 0
                    curr_metrics['node_count'] = node_count

                    if self.node_metrics:
                        latest_node_metrics = {}
                        for metric in self.node_metrics:
                            node_name = metric['name']
                            if node_name not in latest_node_metrics or metric['timestamp'] > latest_node_metrics[node_name]['timestamp']:
                                latest_node_metrics[node_name] = metric

                        ready_nodes = sum(1 for m in latest_node_metrics.values() if m['conditions'].get('Ready') == 'True')
                        curr_metrics['ready_nodes'] = ready_nodes

                    # Pod metrics
                    if self.pod_metrics:
                        latest_pod_metrics = {}
                        for metric in self.pod_metrics:
                            pod_key = f"{metric['namespace']}/{metric['name']}"
                            if pod_key not in latest_pod_metrics or metric['timestamp'] > latest_pod_metrics[pod_key]['timestamp']:
                                latest_pod_metrics[pod_key] = metric

                        curr_metrics['pod_count'] = len(latest_pod_metrics)
                        curr_metrics['running_pods'] = sum(1 for m in latest_pod_metrics.values() if m['phase'] == 'Running')
                        curr_metrics['ready_pods'] = sum(1 for m in latest_pod_metrics.values() if m['ready'])

                    # Component metrics
                    if self.component_metrics:
                        curr_metrics['component_count'] = len(self.component_metrics)
                        healthy_components = 0
                        degraded_components = 0
                        unhealthy_components = 0

                        for component, metrics in self.component_metrics.items():
                            if not metrics:
                                continue

                            # Get latest metric for component
                            latest_metric = max(metrics, key=lambda m: m['timestamp'])

                            if latest_metric['status'] == 'Healthy':
                                healthy_components += 1
                            elif latest_metric['status'] == 'Degraded':
                                degraded_components += 1
                            else:
                                unhealthy_components += 1

                        curr_metrics['healthy_components'] = healthy_components
                        curr_metrics['degraded_components'] = degraded_components
                        curr_metrics['unhealthy_components'] = unhealthy_components

                    # Compare and display changes
                    f.write("Metric Comparison (Previous → Current):\n")

                    # Node comparison with grammatically correct pluralization
                    f.write("\nNodes:\n")
                    prev_node_count = prev_metrics.get('node_count', 0)
                    curr_node_count = curr_metrics.get('node_count', 0)
                    prev_node_word = "node" if prev_node_count == 1 else "nodes"
                    curr_node_word = "node" if curr_node_count == 1 else "nodes"
                    self._write_metric_comparison(f, f"Total {curr_node_word}", prev_metrics.get('node_count'), curr_metrics.get('node_count'))
                    self._write_metric_comparison(f, f"Ready {curr_node_word}", prev_metrics.get('ready_nodes'), curr_metrics.get('ready_nodes'))

                    # Pod comparison with grammatically correct pluralization
                    f.write("\nPods:\n")
                    prev_pod_count = prev_metrics.get('pod_count', 0)
                    curr_pod_count = curr_metrics.get('pod_count', 0)
                    prev_pod_word = "pod" if prev_pod_count == 1 else "pods"
                    curr_pod_word = "pod" if curr_pod_count == 1 else "pods"
                    self._write_metric_comparison(f, f"Total {curr_pod_word}", prev_metrics.get('pod_count'), curr_metrics.get('pod_count'))
                    self._write_metric_comparison(f, f"Running {curr_pod_word}", prev_metrics.get('running_pods'), curr_metrics.get('running_pods'))
                    self._write_metric_comparison(f, f"Ready {curr_pod_word}", prev_metrics.get('ready_pods'), curr_metrics.get('ready_pods'))

                    # Component comparison with grammatically correct pluralization
                    f.write("\nComponents:\n")
                    prev_comp_count = prev_metrics.get('component_count', 0)
                    curr_comp_count = curr_metrics.get('component_count', 0)
                    prev_comp_word = "component" if prev_comp_count == 1 else "components"
                    curr_comp_word = "component" if curr_comp_count == 1 else "components"
                    self._write_metric_comparison(f, f"Total {curr_comp_word}", prev_metrics.get('component_count'), curr_metrics.get('component_count'))
                    self._write_metric_comparison(f, f"Healthy {curr_comp_word}", prev_metrics.get('healthy_components'), curr_metrics.get('healthy_components'))
                    self._write_metric_comparison(f, f"Degraded {curr_comp_word}", prev_metrics.get('degraded_components'), curr_metrics.get('degraded_components'))
                    self._write_metric_comparison(f, f"Unhealthy {curr_comp_word}", prev_metrics.get('unhealthy_components'), curr_metrics.get('unhealthy_components'))

                    f.write("\n")
                else:
                    f.write("Previous summary file not found for detailed comparison\n\n")

                # Compare node metrics if available
                current_nodes = set(metric['name'] for metric in self.node_metrics) if self.node_metrics else set()

                # Try to load previous node metrics
                prev_node_metrics_file = prev_dir / "node-metrics.log"
                if prev_node_metrics_file.exists():
                    prev_nodes = self._extract_nodes_from_metrics(prev_node_metrics_file)

                    f.write("=== Node Changes ===\n")

                    # Identify new and missing nodes
                    new_nodes = current_nodes - prev_nodes
                    missing_nodes = prev_nodes - current_nodes

                    if new_nodes:
                        new_count = len(new_nodes)
                        new_word = "node" if new_count == 1 else "nodes"
                        f.write(f"New {new_word} ({new_count}):\n")
                        for node in sorted(new_nodes):
                            f.write(f"  + {node}\n")
                        f.write("\n")

                    if missing_nodes:
                        missing_count = len(missing_nodes)
                        missing_word = "node" if missing_count == 1 else "nodes"
                        f.write(f"Missing {missing_word} ({missing_count}):\n")
                        for node in sorted(missing_nodes):
                            f.write(f"  - {node}\n")
                        f.write("\n")

                    if not new_nodes and not missing_nodes:
                        f.write("No changes in node membership\n\n")
                else:
                    f.write("Previous node metrics not found for comparison\n\n")

                f.write("=== Comparison Complete ===\n")

            self.logger.info(f"Comparison generated at {comparison_file}")
        except Exception as e:
            self.logger.error(f"Failed to compare with previous results: {e}")

    def compress_results(self) -> None:
        """
        Compress all monitoring results into a single archive file.
        Creates a tar.gz file in the log directory.
        """
        self.logger.info("Compressing monitoring results...")

        try:
            # Archive path - outside the deployment directory
            archive_path = self.log_dir / f"k3s-monitor-{self.deployment_id}.tar.gz"

            # Create the archive
            with tarfile.open(archive_path, "w:gz") as tar:
                # Add all files in the deployment directory
                for item in self.deploy_log_dir.glob("**/*"):
                    if item.is_file():
                        # Add file with relative path
                        rel_path = item.relative_to(self.log_dir)
                        tar.add(item, arcname=str(rel_path))

            # Log file sizes
            archive_size_mb = archive_path.stat().st_size / (1024 * 1024)
            self.logger.info(f"Created archive at {archive_path} ({archive_size_mb:.2f} MB)")

            # Write message to indicate where the results are stored
            with open(self.deploy_log_dir / "README.txt", 'w') as f:
                f.write(f"K3s monitoring results for deployment {self.deployment_id}\n")
                f.write(f"Timestamp: {datetime.datetime.now()}\n\n")
                f.write(f"Full results are available in the compressed archive:\n")
                f.write(f"{archive_path}\n\n")
                f.write(f"Logs directory: {self.deploy_log_dir}\n")

            self.logger.info("Monitoring results compression complete")
        except Exception as e:
            self.logger.error(f"Failed to compress monitoring results: {e}")

    def discover_component_namespaces(self):
        """
        Dynamically discover the namespaces where components are installed.
        Updates the self.component_namespaces dictionary with detected namespaces.
        Automatically skips components that are not found in the cluster.
        """
        self.logger.info("Discovering component namespaces...")

        try:
            # Get all namespaces
            namespaces = [ns.metadata.name for ns in self.k8s_api.list_namespace().items]
            namespace_count = len(namespaces)
            namespace_word = "namespace" if namespace_count == 1 else "namespaces"
            self.logger.info(f"Found {namespace_count} {namespace_word}")

            # Get all pods across all namespaces
            pods = self.k8s_api.list_pod_for_all_namespaces(watch=False)

            # Map to track components found
            found_components = {}

            # First check using pod names
            for pod in pods.items:
                pod_name = pod.metadata.name
                pod_namespace = pod.metadata.namespace

                for component, prefixes in self.config.components.items():
                    if component in found_components:
                        continue

                    for prefix in prefixes:
                        if prefix in pod_name.lower():
                            found_components[component] = pod_namespace
                            self.logger.info(f"Found component {component} in namespace {pod_namespace} via pod {pod_name}")
                            break

            # If not all components found, try using labels
            if len(found_components) < len(self.config.components):
                for pod in pods.items:
                    pod_name = pod.metadata.name
                    pod_namespace = pod.metadata.namespace
                    labels = pod.metadata.labels or {}

                    for component, prefixes in self.config.components.items():
                        if component in found_components:
                            continue

                        # Check if any common labels match
                        app_name = self.labels.get_label('APP_KUBERNETES_NAME')
                        app_instance = self.labels.get_label('APP_KUBERNETES_INSTANCE')

                        if app_name and labels.get(app_name) == component:
                            found_components[component] = pod_namespace
                            self.logger.info(f"Found component {component} in namespace {pod_namespace} via pod label {app_name}")
                            break
                        elif app_instance and labels.get(app_instance) == component:
                            found_components[component] = pod_namespace
                            self.logger.info(f"Found component {component} in namespace {pod_namespace} via pod label {app_instance}")
                            break

            # If still not all components found, try using deployments, daemonsets, and statefulsets
            if len(found_components) < len(self.config.components):
                # Check deployments
                for ns in namespaces:
                    deployments = self.k8s_apps_api.list_namespaced_deployment(namespace=ns)
                    for deploy in deployments.items:
                        deploy_name = deploy.metadata.name

                        for component, prefixes in self.config.components.items():
                            if component in found_components:
                                continue

                            for prefix in prefixes:
                                if prefix in deploy_name.lower():
                                    found_components[component] = ns
                                    self.logger.info(f"Found component {component} in namespace {ns} via deployment {deploy_name}")
                                    break

                # Check daemonsets
                for ns in namespaces:
                    daemonsets = self.k8s_apps_api.list_namespaced_daemon_set(namespace=ns)
                    for ds in daemonsets.items:
                        ds_name = ds.metadata.name

                        for component, prefixes in self.config.components.items():
                            if component in found_components:
                                continue

                            for prefix in prefixes:
                                if prefix in ds_name.lower():
                                    found_components[component] = ns
                                    self.logger.info(f"Found component {component} in namespace {ns} via daemonset {ds_name}")
                                    break

                # Check statefulsets
                for ns in namespaces:
                    statefulsets = self.k8s_apps_api.list_namespaced_stateful_set(namespace=ns)
                    for sts in statefulsets.items:
                        sts_name = sts.metadata.name

                        for component, prefixes in self.config.components.items():
                            if component in found_components:
                                continue

                            for prefix in prefixes:
                                if prefix in sts_name.lower():
                                    found_components[component] = ns
                                    self.logger.info(f"Found component {component} in namespace {ns} via statefulset {sts_name}")
                                    break

            # Identify components that were not found in the cluster
            not_found_components = set(self.config.components.keys()) - set(found_components.keys())

            # Log and automatically skip components not found in the cluster
            if not_found_components:
                not_found_count = len(not_found_components)
                component_word = "component" if not_found_count == 1 else "components"
                self.logger.info(f"The following {component_word} were not found in the cluster and will be skipped: {', '.join(not_found_components)}")

                # Remove components that aren't found from self.components
                for component in not_found_components:
                    del self.config.components[component]

            # Set the component_namespaces dict to only include found components
            self.component_namespaces = found_components

            self.logger.info("Component namespace discovery completed")

            found_components_count = len(self.component_namespaces)
            component_word = "component" if found_components_count == 1 else "components"
            self.logger.info(f"Monitoring these {component_word}: {', '.join(self.component_namespaces.keys())}")

        except Exception as e:
            self.logger.error(f"Error discovering component namespaces: {e}")
            # Use empty dict if discovery fails - we'll only monitor what we can find
            self.component_namespaces = {}
            self.logger.warning("Using empty component list due to discovery failure")
            import traceback
            self.logger.error(traceback.format_exc())

    def exec_in_pod(self, namespace: str, pod_name: str, command: List[str]) -> Tuple[bool, str]:
        """
        Execute a command in a specific pod.

        Args:
            namespace: Pod namespace
            pod_name: Pod name
            command: Command to execute as list of strings

        Returns:
            Tuple of (success, output)
        """
        try:
            if not pod_name:
                return False, "No pod name provided"

            # Construct kubectl exec command
            kubectl_command = ["kubectl", "exec", "-n", namespace, pod_name, "--"]
            kubectl_command.extend(command)

            return self.run_kubectl_command(kubectl_command)

        except Exception as e:
            self.logger.error(f"Error executing command in pod: {e}")
            return False, str(e)

    def exec_in_pod_by_label(self, namespace: str, label_selector: str, command: List[str]) -> Tuple[bool, str]:
        """
        Execute a command in a pod selected by label.

        Args:
            namespace: Pod namespace
            label_selector: Label selector in format "key=value"
            command: Command to execute as list of strings

        Returns:
            Tuple of (success, output)
        """
        pod_name = self.get_pod_by_label(namespace, label_selector)
        if pod_name:
            return self.exec_in_pod(namespace, pod_name, command)
        else:
            return False, f"No pod found matching label {label_selector} in namespace {namespace}"

    def generate_log_summary(self) -> None:
        """
        Generate a summary of collected log files with key statistics.
        Creates a log-summary.log file in the deployment directory.
        """
        self.logger.info("Generating log summary...")

        try:
            log_summary_file = self.deploy_log_dir / "log-summary.log"

            with open(log_summary_file, 'w') as f:
                f.write("=== K3s Log Summary ===\n")
                f.write(f"Timestamp: {datetime.datetime.now()}\n")
                f.write(f"Deployment ID: {self.deployment_id}\n\n")

                # Count and summarize service logs
                service_logs = defaultdict(list)
                for log_file in self.service_logs_dir.glob("*.log"):
                    # Extract service name from filename pattern: service-timestamp.log
                    parts = log_file.stem.split('-')
                    if len(parts) >= 2:
                        service_name = parts[0]
                        service_logs[service_name].append(log_file)

                service_count = len(service_logs)
                service_word = "service" if service_count == 1 else "services"
                f.write("=== Service Logs Summary ===\n")
                f.write(f"{service_word.capitalize()} monitored: {service_count}\n\n")

                for service, logs in service_logs.items():
                    log_count = len(logs)
                    log_word = "log file" if log_count == 1 else "log files"
                    f.write(f"Service: {service}\n")
                    f.write(f"  {log_word.capitalize()}: {log_count}\n")

                    # Get statistics for the most recent log file
                    if logs:
                        latest_log = max(logs, key=lambda f: f.stat().st_mtime)

                        # Get file size
                        size_bytes = latest_log.stat().st_size
                        size_kb = size_bytes / 1024

                        f.write(f"  Latest log: {latest_log.name}\n")
                        f.write(f"  Size: {size_kb:.2f} KB\n")

                        # Count lines and errors in the log file
                        try:
                            with open(latest_log, 'r', errors='replace') as log_f:
                                lines = log_f.readlines()

                                total_lines = len(lines)
                                error_lines = sum(1 for line in lines if 'error' in line.lower())
                                warning_lines = sum(1 for line in lines if 'warn' in line.lower() and 'error' not in line.lower())

                                line_word = "line" if total_lines == 1 else "lines"
                                error_word = "error" if error_lines == 1 else "errors"
                                warning_word = "warning" if warning_lines == 1 else "warnings"

                                f.write(f"  {line_word.capitalize()}: {total_lines}\n")
                                f.write(f"  {error_word.capitalize()}: {error_lines}\n")
                                f.write(f"  {warning_word.capitalize()}: {warning_lines}\n")

                                # Extract the last 3 errors for context
                                if error_lines > 0:
                                    f.write("  Recent errors:\n")
                                    error_count = 0
                                    for line in reversed(lines):
                                        if 'error' in line.lower():
                                            f.write(f"    {line.strip()}\n")
                                            error_count += 1
                                            if error_count >= 3:
                                                break
                        except Exception as e:
                            f.write(f"  Error analyzing log: {str(e)}\n")

                    f.write("\n")

                # Count and summarize Kubernetes logs
                k8s_logs_dir = self.service_logs_dir / "k8s"
                if k8s_logs_dir.exists() and k8s_logs_dir.is_dir():
                    k8s_logs = defaultdict(list)
                    for log_file in k8s_logs_dir.glob("*.log"):
                        # Extract component name from filename pattern: component-timestamp.log
                        parts = log_file.stem.split('-')
                        if len(parts) >= 2:
                            component_name = parts[0]
                            k8s_logs[component_name].append(log_file)

                    component_count = len(k8s_logs)
                    component_word = "component" if component_count == 1 else "components"
                    f.write("=== Kubernetes Components Logs Summary ===\n")
                    f.write(f"{component_word.capitalize()} with logs: {component_count}\n\n")

                    for component, logs in k8s_logs.items():
                        log_count = len(logs)
                        log_word = "log file" if log_count == 1 else "log files"
                        f.write(f"Component: {component}\n")
                        f.write(f"  {log_word.capitalize()}: {log_count}\n")

                        # Get statistics for the most recent log file
                        if logs:
                            latest_log = max(logs, key=lambda f: f.stat().st_mtime)

                            # Get file size
                            size_bytes = latest_log.stat().st_size
                            size_kb = size_bytes / 1024

                            f.write(f"  Latest log: {latest_log.name}\n")
                            f.write(f"  Size: {size_kb:.2f} KB\n")

                            # Count lines and errors in the log file
                            try:
                                with open(latest_log, 'r', errors='replace') as log_f:
                                    lines = log_f.readlines()

                                    total_lines = len(lines)
                                    error_lines = sum(1 for line in lines if 'error' in line.lower())
                                    warning_lines = sum(1 for line in lines if 'warn' in line.lower() and 'error' not in line.lower())

                                    line_word = "line" if total_lines == 1 else "lines"
                                    error_word = "error" if error_lines == 1 else "errors"
                                    warning_word = "warning" if warning_lines == 1 else "warnings"

                                    f.write(f"  {line_word.capitalize()}: {total_lines}\n")
                                    f.write(f"  {error_word.capitalize()}: {error_lines}\n")
                                    f.write(f"  {warning_word.capitalize()}: {warning_lines}\n")

                                    # Extract the last 3 errors for context
                                    if error_lines > 0:
                                        f.write("  Recent errors:\n")
                                        error_count = 0
                                        for line in reversed(lines):
                                            if 'error' in line.lower():
                                                f.write(f"    {line.strip()}\n")
                                                error_count += 1
                                                if error_count >= 3:
                                                    break
                            except Exception as e:
                                f.write(f"  Error analyzing log: {str(e)}\n")

                        f.write("\n")

                # Add log file sizes
                f.write("=== Collected Log Files ===\n")

                # Dictionary to store log files by type
                log_files_by_type = defaultdict(list)

                # Function to categorize log files
                def categorize_file(file_path):
                    if file_path.suffix != '.log':
                        return 'other'
                    if file_path.parent == self.service_logs_dir:
                        return 'service'
                    if file_path.parent == (self.service_logs_dir / "k8s"):
                        return 'kubernetes'
                    if file_path.parent == self.deploy_log_dir:
                        if file_path.name.startswith('node-'):
                            return 'node'
                        if file_path.name.startswith('pod-'):
                            return 'pod'
                        if file_path.name.startswith('component-'):
                            return 'component'
                        if file_path.name == 'k3s-monitor.log':
                            return 'monitor'
                        return 'summary'
                    return 'other'

                # Find all log files in the deployment directory and subdirectories
                for log_file in self.deploy_log_dir.glob("**/*.log"):
                    log_type = categorize_file(log_file)
                    log_files_by_type[log_type].append(log_file)

                # Write summary by log type
                for log_type, files in log_files_by_type.items():
                    file_count = len(files)
                    file_word = "file" if file_count == 1 else "files"
                    total_size = sum(f.stat().st_size for f in files)
                    total_size_kb = total_size / 1024

                    f.write(f"{log_type.capitalize()} logs: {file_count} {file_word}, {total_size_kb:.2f} KB\n")

                # Calculate total log size
                all_files = list(self.deploy_log_dir.glob("**/*"))
                total_file_count = len(all_files)
                file_word = "file" if total_file_count == 1 else "files"
                total_size = sum(f.stat().st_size for f in all_files if f.is_file())
                total_size_mb = total_size / (1024 * 1024)

                f.write(f"\nTotal collected logs: {total_file_count} {file_word}, {total_size_mb:.2f} MB\n")

                f.write("\n=== Log Summary Complete ===\n")

            self.logger.info(f"Log summary generated at {log_summary_file}")
        except Exception as e:
            self.logger.error(f"Failed to generate log summary: {e}")

    def generate_summary(self) -> None:
        """
        Generate a summary report of the monitoring session.
        Creates a summary.log file in the deployment directory.
        """
        self.logger.info("Generating monitoring summary...")

        try:
            summary_file = self.deploy_log_dir / "summary.log"

            with open(summary_file, 'w') as f:
                f.write("=== K3s Monitoring Summary ===\n")
                f.write(f"Timestamp: {datetime.datetime.now()}\n")
                f.write(f"Deployment ID: {self.deployment_id}\n")
                f.write(f"Duration: {self.duration_seconds} seconds\n")
                f.write(f"Interval: {self.interval_seconds} seconds\n\n")

                # Cluster architecture information
                f.write("=== Cluster Architecture ===\n")
                f.write(f"Cluster type: {getattr(self, 'cluster_type', 'Unknown')}\n")
                f.write(f"High Availability: {getattr(self, 'is_ha_cluster', False)}\n")

                if hasattr(self, 'architecture_details'):
                    arch_details = self.architecture_details
                    f.write(f"Datastore type: {arch_details.get('datastore_type', 'Unknown')}\n")

                    # Correctly handle singular/plural for control plane nodes
                    cp_node_count = len(arch_details.get('control_plane_nodes', []))
                    cp_node_word = "node" if cp_node_count == 1 else "nodes"
                    f.write(f"Control plane {cp_node_word}: {cp_node_count}\n")

                    # Correctly handle singular/plural for worker nodes
                    worker_node_count = len(arch_details.get('worker_nodes', []))
                    worker_node_word = "node" if worker_node_count == 1 else "nodes"
                    f.write(f"Worker {worker_node_word}: {worker_node_count}\n\n")

                # Node summary
                node_count = len(set(metric['name'] for metric in self.node_metrics)) if self.node_metrics else 0
                node_word = "node" if node_count == 1 else "nodes"
                f.write("=== Node Summary ===\n")
                f.write(f"Total {node_word}: {node_count}\n")

                if self.node_metrics:
                    # Get latest node metrics
                    latest_metrics = {}
                    for metric in self.node_metrics:
                        node_name = metric['name']
                        if node_name not in latest_metrics or metric['timestamp'] > latest_metrics[node_name]['timestamp']:
                            latest_metrics[node_name] = metric

                    # Summarize node status
                    ready_nodes = sum(1 for m in latest_metrics.values() if m['conditions'].get('Ready') == 'True')
                    f.write(f"Ready {node_word}: {ready_nodes}/{node_count}\n\n")

                    # List nodes with issues
                    nodes_with_issues = [name for name, m in latest_metrics.items() if m['conditions'].get('Ready') != 'True']
                    if nodes_with_issues:
                        issue_count = len(nodes_with_issues)
                        issue_word = "node" if issue_count == 1 else "nodes"
                        f.write(f"{issue_word.capitalize()} with issues:\n")
                        for node_name in nodes_with_issues:
                            f.write(f"  - {node_name}\n")
                    f.write("\n")

                # Pod summary
                if self.pod_metrics:
                    # Get latest pod metrics
                    latest_pod_metrics = {}
                    for metric in self.pod_metrics:
                        pod_key = f"{metric['namespace']}/{metric['name']}"
                        if pod_key not in latest_pod_metrics or metric['timestamp'] > latest_pod_metrics[pod_key]['timestamp']:
                            latest_pod_metrics[pod_key] = metric

                    # Group pods by namespace
                    pods_by_namespace = defaultdict(list)
                    for metric in latest_pod_metrics.values():
                        pods_by_namespace[metric['namespace']].append(metric)

                    pod_count = len(latest_pod_metrics)
                    pod_word = "pod" if pod_count == 1 else "pods"
                    f.write("=== Pod Summary ===\n")
                    f.write(f"Total {pod_word}: {pod_count}\n")

                    # Count pods by status
                    running_pods = sum(1 for m in latest_pod_metrics.values() if m['phase'] == 'Running')
                    ready_pods = sum(1 for m in latest_pod_metrics.values() if m['ready'])
                    f.write(f"Running {pod_word}: {running_pods}/{pod_count}\n")
                    f.write(f"Ready {pod_word}: {ready_pods}/{pod_count}\n\n")

                    # Report pods with issues
                    pods_with_issues = []
                    for pod_key, metric in latest_pod_metrics.items():
                        if metric['phase'] != 'Running' or not metric['ready'] or metric['restart_count'] > 5:
                            pods_with_issues.append((pod_key, metric))

                    if pods_with_issues:
                        issue_count = len(pods_with_issues)
                        issue_word = "pod" if issue_count == 1 else "pods"
                        f.write(f"{issue_word.capitalize()} with issues:\n")
                        for pod_key, metric in pods_with_issues:
                            f.write(f"  - {pod_key} (Phase: {metric['phase']}, Ready: {metric['ready']}, Restarts: {metric['restart_count']})\n")
                    f.write("\n")

                # Component summary
                if self.component_metrics:
                    component_count = len(self.component_metrics)
                    component_word = "component" if component_count == 1 else "components"
                    f.write("=== Component Summary ===\n")
                    f.write(f"Monitored {component_word}: {component_count}\n\n")

                    for component, metrics in self.component_metrics.items():
                        if not metrics:
                            continue

                        # Get latest metric for component
                        latest_metric = max(metrics, key=lambda m: m['timestamp'])

                        f.write(f"Component: {component}\n")
                        f.write(f"  Namespace: {latest_metric['namespace']}\n")
                        f.write(f"  Status: {latest_metric['status']}\n")

                        # Use grammatically correct form for pod count
                        pod_count = latest_metric['pod_count']
                        pod_word = "pod" if pod_count == 1 else "pods"
                        f.write(f"  Healthy {pod_word}: {latest_metric['healthy_pods']}/{pod_count}\n")
                        f.write("\n")

                f.write("=== Monitoring Complete ===\n")

            self.logger.info(f"Summary generated at {summary_file}")
        except Exception as e:
            self.logger.error(f"Failed to generate summary: {e}")

    def get_pod_by_label(self, namespace: str, label_selector: str) -> str:
        """
        Get a pod name using a label selector.

        Args:
            namespace: Namespace to search in
            label_selector: Label selector in format "key=value"

        Returns:
            Pod name if found, empty string otherwise
        """
        try:
            self.logger.debug(f"Getting pod by label selector {label_selector} in namespace {namespace}")

            # Use kubectl to get the pod name - removed the --limit flag
            success, output = self.run_kubectl_command([
                "kubectl", "get", "pods", "-n", namespace,
                "-l", label_selector, "-o", "name"
            ])

            if success and output.strip():
                # Extract pod name from the output (format will be "pod/name")
                pod_name = output.strip().split('\n')[0]  # Take first pod if multiple are returned

                # Remove "pod/" prefix if present
                if pod_name.startswith("pod/"):
                    pod_name = pod_name[4:]

                self.logger.debug(f"Found pod {pod_name}")
                return pod_name
            else:
                self.logger.debug(f"No pods found matching label {label_selector} in namespace {namespace}")
                return ""

        except Exception as e:
            self.logger.error(f"Error getting pod by label: {e}")
            return ""

    def log_cluster_info(self):
        """Collect and log basic cluster information with architecture details."""
        info_file = self.config.deploy_log_dir / "cluster-info.log"

        try:
            # Get nodes information
            nodes = self.k8s_api.list_node()
            node_count = len(nodes.items)
            node_word = "node" if node_count == 1 else "nodes"

            # Get K3s version from the first node
            k3s_version = nodes.items[0].status.node_info.kubelet_version if nodes.items else "Unknown"

            # Identify control plane and worker nodes
            control_plane_nodes = []
            worker_nodes = []

            for node in nodes.items:
                node_info = {
                    "name": node.metadata.name,
                    "status": node.status.conditions[-1].type,
                    "kubelet_version": node.status.node_info.kubelet_version,
                    "os_image": node.status.node_info.os_image,
                    "kernel_version": node.status.node_info.kernel_version,
                    "addresses": [addr.address for addr in node.status.addresses]
                }

                is_control_plane = False
                for selector in self.labels.get_control_plane_selectors():
                    key, value = selector.split('=')
                    if node.metadata.labels.get(key) == value:
                        is_control_plane = True
                        break

                if is_control_plane:
                    control_plane_nodes.append(node_info)
                else:
                    worker_nodes.append(node_info)

            cp_node_count = len(control_plane_nodes)
            cp_node_word = "node" if cp_node_count == 1 else "nodes"

            worker_node_count = len(worker_nodes)
            worker_node_word = "node" if worker_node_count == 1 else "nodes"

            # Get deployments, daemonsets and statefulsets in kube-system
            api_apps = client.AppsV1Api()
            deployments = api_apps.list_namespaced_deployment(namespace=self.config.default_namespace)
            daemonsets = api_apps.list_namespaced_daemon_set(namespace=self.config.default_namespace)
            statefulsets = api_apps.list_namespaced_stateful_set(namespace=self.config.default_namespace)

            # Write information to file
            with open(info_file, 'w') as f:
                f.write("=== Cluster Information ===\n")
                f.write(f"Timestamp: {datetime.datetime.now()}\n")

                # Add architecture information
                f.write("\n=== Architecture Information ===\n")
                f.write(f"Cluster type: {self.cluster_type}\n")
                f.write(f"High Availability: {self.is_ha_cluster}\n")
                f.write(f"Datastore type: {self.architecture_details.get('datastore_type', 'Unknown')}\n")

                if self.architecture_details.get('datastore_endpoint'):
                    # Mask sensitive information in connection strings
                    endpoint = self.architecture_details.get('datastore_endpoint')
                    if "://" in endpoint and "@" in endpoint:
                        # Simple masking for DB connection strings
                        parts = endpoint.split("@")
                        masked_endpoint = f"{parts[0].split('://')[0]}://*****@{parts[1]}"
                        f.write(f"Datastore endpoint: {masked_endpoint}\n")
                    else:
                        f.write(f"Datastore endpoint: {endpoint}\n")

                f.write(f"Control plane {cp_node_word}: {cp_node_count}\n")
                f.write(f"Worker {worker_node_word}: {worker_node_count}\n")

                if self.architecture_details.get('is_etcd_embedded'):
                    f.write("Using embedded etcd for high availability\n")
                elif self.architecture_details.get('external_db_type'):
                    f.write(f"Using external {self.architecture_details.get('external_db_type')} database\n")

                # Get OS information using platform module
                try:
                    # Get Linux distribution info
                    linux_dist = platform.freedesktop_os_release() if hasattr(platform, 'freedesktop_os_release') else {}
                    os_name = linux_dist.get('NAME', platform.system())
                    os_version = linux_dist.get('VERSION', platform.release())

                    if os_name and os_version:
                        f.write(f"\nOS: {os_name} {os_version}\n")
                    else:
                        # Fallback to platform.platform() if specific info not found
                        f.write(f"\nOS: {platform.platform()}\n")
                except Exception as e:
                    self.logger.error(f"Failed to get OS version: {e}")
                    f.write(f"\nOS: Unknown (error retrieving info)\n")

                f.write(f"K3s Version: {k3s_version}\n")
                f.write(f"Number of {node_word}: {node_count}\n\n")

                f.write("=== Control Plane Nodes ===\n")
                for node in control_plane_nodes:
                    f.write(f"Name: {node['name']}\n")
                    f.write(f"Status: {node['status']}\n")
                    f.write(f"Addresses: {', '.join(node['addresses'])}\n\n")

                f.write("=== Worker Nodes ===\n")
                for node in worker_nodes:
                    f.write(f"Name: {node['name']}\n")
                    f.write(f"Status: {node['status']}\n")
                    f.write(f"Addresses: {', '.join(node['addresses'])}\n\n")

                # Deployments with correct pluralization
                deploy_count = len(deployments.items)
                deploy_word = "Deployment" if deploy_count == 1 else "Deployments"
                f.write(f"=== {deploy_word} in kube-system ===\n")
                for deploy in deployments.items:
                    f.write(f"Name: {deploy.metadata.name}, Replicas: {deploy.status.ready_replicas}/{deploy.status.replicas}\n")
                f.write("\n")

                # DaemonSets with correct pluralization
                ds_count = len(daemonsets.items)
                ds_word = "DaemonSet" if ds_count == 1 else "DaemonSets"
                f.write(f"=== {ds_word} in kube-system ===\n")
                for ds in daemonsets.items:
                    f.write(f"Name: {ds.metadata.name}, Desired: {ds.status.desired_number_scheduled}, Ready: {ds.status.number_ready}\n")
                f.write("\n")

                # StatefulSets with correct pluralization
                sts_count = len(statefulsets.items)
                sts_word = "StatefulSet" if sts_count == 1 else "StatefulSets"
                f.write(f"=== {sts_word} in kube-system ===\n")
                for sts in statefulsets.items:
                    f.write(f"Name: {sts.metadata.name}, Replicas: {sts.status.ready_replicas}/{sts.status.replicas if sts.status.replicas else 0}\n")
                f.write("\n")

                f.write("=== Sysctl Optimizations ===\n")
                f.write("Standard optimized parameters are applied on all nodes.\n")
                f.write("Cilium-specific sysctl overrides are also applied.\n")

            self.logger.info(f"Cluster information logged to {info_file}")
        except Exception as e:
            self.logger.error(f"Failed to log cluster information: {e}")
            import traceback
            self.logger.error(traceback.format_exc())

    def manage_log_storage(self) -> None:
        """
        Manage log storage to prevent excessive disk usage.
        Compresses or removes old log files based on size limits.
        """
        self.logger.debug("Managing log storage...")

        try:
            # Check service logs directory size
            service_logs_size = 0
            for log_file in self.service_logs_dir.glob("**/*"):
                if log_file.is_file():
                    service_logs_size += log_file.stat().st_size

            # Convert to MB
            service_logs_size_mb = service_logs_size / (1024 * 1024)

            # If logs exceed the limit, compress the oldest logs
            if service_logs_size_mb > self.log_max_size:
                self.logger.info(f"Service logs size ({service_logs_size_mb:.2f} MB) exceeds limit ({self.log_max_size} MB)")

                # Get all log files sorted by creation time
                log_files = sorted(
                    [f for f in self.service_logs_dir.glob("**/*") if f.is_file()],
                    key=lambda f: f.stat().st_ctime
                )

                # Compress oldest logs until under size limit
                files_to_compress = []
                compressed_size = 0
                for log_file in log_files:
                    file_size_mb = log_file.stat().st_size / (1024 * 1024)
                    files_to_compress.append(log_file)
                    compressed_size += file_size_mb

                    if service_logs_size_mb - compressed_size < self.log_max_size:
                        break

                # Create a compressed archive of old logs
                if files_to_compress:
                    self.logger.info(f"Compressing {len(files_to_compress)} old log files")

                    # Create a tar archive in the deploy log dir
                    archive_path = self.deploy_log_dir / f"old-logs-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.tar.gz"

                    with tarfile.open(archive_path, "w:gz") as tar:
                        for log_file in files_to_compress:
                            # Add relative path to tar
                            rel_path = log_file.relative_to(self.service_logs_dir)
                            tar.add(log_file, arcname=str(rel_path))

                    # Remove the original files
                    for log_file in files_to_compress:
                        log_file.unlink()

                    self.logger.info(f"Compressed old logs to {archive_path}")

            self.logger.debug("Log storage management completed")
        except Exception as e:
            self.logger.error(f"Failed to manage log storage: {e}")

    def run_kubectl_command(self, command):
        """
        Run a kubectl command and return the output.

        Args:
            command: List of command parts to run

        Returns:
            Tuple of (success, output)
        """
        try:
            result = subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True
            )
            return True, result.stdout
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Command failed: {' '.join(command)}")
            self.logger.error(f"Error: {e.stderr}")
            return False, e.stderr
        except Exception as e:
            self.logger.error(f"Failed to run command: {' '.join(command)}")
            self.logger.error(f"Error: {str(e)}")
            return False, str(e)

    def run(self):
        """Run the monitoring process."""
        # Check prerequisites
        if not self.check_prerequisites():
            self.logger.error("Failed prerequisite check. Exiting.")
            return

        # Log cluster information
        self.log_cluster_info()

        self.logger.info(f"Starting K3s cluster resource monitoring...")
        self.logger.info(f"Logs will be stored in {self.config.deploy_log_dir}")
        self.logger.info(f"Deployment ID: {self.config.deployment_id}")

        # Log architecture info if available
        self.logger.info(f"Cluster architecture: {self.cluster_type}")
        self.logger.info(f"High Availability mode: {'Yes' if self.is_ha_cluster else 'No'}")

        iterations = self.config.duration_seconds // self.config.interval_seconds
        for i in range(1, iterations + 1):
            self.logger.info(f"Collecting metrics (iteration {i} of {iterations})...")

            # Clear pod metrics between iterations to avoid duplicates
            self.pod_metrics = []

            # Collect metrics - FIXED ORDER: nodes, pods, then components
            self.collect_node_metrics()
            self.collect_pod_metrics()
            self.collect_component_metrics()

            # Architecture-specific monitoring
            if self.is_ha_cluster:
                self.collect_etcd_metrics()
            self.collect_cilium_metrics()

            # Collect logs aligned with metrics
            self.collect_service_logs()
            self.collect_kubernetes_logs()

            # Manage log storage to prevent excessive disk usage
            self.manage_log_storage()

            # Sleep until next interval if not the last iteration
            if i < iterations:
                time.sleep(self.config.interval_seconds)

        # Generate reports
        self.generate_summary()
        self.compare_with_previous()
        self.generate_log_summary()

        self.logger.info(f"Monitoring completed. Results stored in {self.config.deploy_log_dir}")
        self.logger.info(f"Summary log: {self.config.deploy_log_dir / 'summary.log'}")

        # Compress the results directory
        self.compress_results()


class K3sMonitorConfig:
    """Configuration class for K3s Monitor."""

    def __init__(self, **kwargs):
        """
        Initialize the K3s monitoring configuration.

        Keyword Arguments:
            duration_seconds: Total monitoring duration in seconds
            interval_seconds: Time between metric collections in seconds
            log_dir: Directory to store logs and reports
            log_max_size: Maximum log file size in MB
            detect_architecture: Enable architecture detection
            verbose: Enable verbose logging
            default_namespace: Default namespace for Kubernetes operations
            custom_labels: Dictionary of custom labels to include
            skip_components: List of components to skip monitoring
            additional_components: Dictionary of additional components to monitor
        """
        # Core configuration parameters
        self.duration_seconds = kwargs.get('duration_seconds', 3600)
        self.interval_seconds = kwargs.get('interval_seconds', 300)
        self.log_dir = Path(kwargs.get('log_dir', "/var/log/k3s"))
        self.log_max_size = kwargs.get('log_max_size', 50)
        self.detect_architecture = kwargs.get('detect_architecture', True)
        self.verbose = kwargs.get('verbose', False)
        self.default_namespace = kwargs.get('default_namespace', "kube-system")

        # Component configuration
        self.custom_labels = kwargs.get('custom_labels', {})
        self.skip_components = kwargs.get('skip_components', [])
        self.additional_components = kwargs.get('additional_components', {})

        # Deployment identifier
        self.deployment_id = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

        # Default components to monitor
        self.components = {
            "argo-cd": ["argo-cd", "argocd"],
            "cert-manager": ["cert-manager"],
            "cilium": ["cilium"],
            "coredns": ["coredns", "kube-dns"],
            "external-dns": ["external-dns"],
            "kured": ["kured"],
            "longhorn": ["longhorn"],
            "metrics-server": ["metrics-server"],
            "victorialogs": ["vls", "victoria-logs-single"],
            "victoriametrics": ["vmks", "victoria-metrics-k8s-stack"]
        }

        # Skip configured components
        for component in self.skip_components:
            if component in self.components:
                del self.components[component]

        # Add additional components
        self.components.update(self.additional_components)

        # Default services to collect logs from
        self.log_services = [
            "k3s",
            "kubelet",
            "containerd"
        ]

        # Validate configuration
        self._validate()

        # Create directory structure
        self._create_directories()

    def _create_directories(self):
        """Create necessary directories for logs and reports."""
        # Create deployment directory
        self.deploy_log_dir = self.log_dir / self.deployment_id
        self.deploy_log_dir.mkdir(parents=True, exist_ok=True)

        # Create service logs directory
        self.service_logs_dir = self.deploy_log_dir / "service"
        self.service_logs_dir.mkdir(parents=True, exist_ok=True)

    def _validate(self):
        """Validate configuration parameters."""
        if self.duration_seconds < 60:
            raise ValueError("duration_seconds must be at least 60 seconds")
        if self.interval_seconds < 10:
            raise ValueError("interval_seconds must be at least 10 seconds")
        if self.interval_seconds > self.duration_seconds:
            raise ValueError("interval_seconds cannot be greater than duration_seconds")

    @classmethod
    def from_args(cls, args):
        """
        Create a configuration from command line arguments.

        Args:
            args: Parsed command line arguments

        Returns:
            K3sMonitorConfig: Configuration populated from command line arguments
        """
        return cls(
            duration_seconds=args.duration,
            interval_seconds=args.interval,
            log_dir=args.log_dir,
            log_max_size=args.log_max_size,
            default_namespace=args.namespace,
            verbose=args.verbose,
            detect_architecture=not args.no_arch_detect
        )

    @classmethod
    def from_env_vars(cls):
        """
        Create a configuration from environment variables.

        Returns:
            K3sMonitorConfig: Configuration populated from environment variables
        """
        config = {}

        # Map environment variables to config parameters
        env_map = {
            'K3S_MONITOR_DURATION': ('duration_seconds', int),
            'K3S_MONITOR_INTERVAL': ('interval_seconds', int),
            'K3S_MONITOR_LOG_DIR': ('log_dir', str),
            'K3S_MONITOR_LOG_MAX_SIZE': ('log_max_size', int),
            'K3S_MONITOR_NAMESPACE': ('default_namespace', str),
            'K3S_MONITOR_VERBOSE': ('verbose', lambda v: v.lower() == 'true'),
            'K3S_MONITOR_DETECT_ARCH': ('detect_architecture', lambda v: v.lower() == 'true')
        }

        # Process environment variables
        for env_var, (config_key, converter) in env_map.items():
            if env_var in os.environ:
                try:
                    config[config_key] = converter(os.environ[env_var])
                except Exception as e:
                    print(f"Warning: Could not convert environment variable {env_var}: {e}")

        return cls(**config)


def main():
    """Parse arguments and start monitoring."""
    parser = argparse.ArgumentParser(
        description='K3s Cluster Monitor',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('-c', '--config-from-env', action='store_true',
                        help='Load configuration from environment variables')
    parser.add_argument('-d', '--duration', type=int, default=3600,
                        help='Total monitoring duration in seconds')
    parser.add_argument('-i', '--interval', type=int, default=300,
                        help='Time between metric collections in seconds')
    parser.add_argument('-l', '--log-dir', type=str, default='/var/log/k3s',
                        help='Directory to store logs and reports')
    parser.add_argument('-m', '--log-max-size', type=int, default=50,
                        help='Maximum log file size in MB')
    parser.add_argument('-n', '--namespace', type=str, default='kube-system',
                        help='Default namespace')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Enable verbose logging')
    parser.add_argument('-x', '--no-arch-detect', action='store_true',
                        help='Disable architecture detection')

    args = parser.parse_args()

    try:
        # Create configuration
        if args.config_from_env:
            config = K3sMonitorConfig.from_env_vars()
        else:
            config = K3sMonitorConfig.from_args(args)

        # Create and initialize the monitor with the provided configuration
        monitor = K3sMonitor(config=config)

        # Run the monitoring process
        monitor.run()
        return 0

    except Exception as e:
        print(f"Error during monitoring: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())
